{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b570a7",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "filename = 'NorthSeaData/FORCE_2020_train.csv' # nome do dado de entrada\n",
    "df = pd.read_csv(filename) # leitura do dado de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca095dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### number feature (well log profiling) visualization\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata \n",
    "training_data = df[['WELL', \n",
    "                    'DEPTH_MD', \n",
    "                    'RMED', \n",
    "                    'RDEP', \n",
    "                    'RHOB', \n",
    "                    'GR', \n",
    "                    'NPHI',\n",
    "                    'DTC', \n",
    "                    'PEF', \n",
    "                    'FORCE_2020_LITHOFACIES_LITHOLOGY']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata to use \n",
    "df = df[['WELL', \n",
    "         'DEPTH_MD', \n",
    "         'RMED', \n",
    "         'RDEP', \n",
    "         'RHOB', \n",
    "         'GR', \n",
    "         'NPHI',\n",
    "         'DTC', \n",
    "         'PEF', \n",
    "         'FORCE_2020_LITHOFACIES_LITHOLOGY']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccee374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the number of wells\n",
    "for well in df['WELL'].unique():\n",
    "    \n",
    "    print(well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ba12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of headers\n",
    "plot_cols = ['WELL', 'DEPTH_MD','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI',\n",
    "             'DTC', 'PEF','FORCE_2020_LITHOFACIES_LITHOLOGY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[plot_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, col in enumerate(data_nan.columns[2:]):\n",
    "    data_nan[col] = data_nan[col].notnull() * (num + 1)\n",
    "    data_nan[col].replace(0, num, inplace=True)\n",
    "    print(col, num) #Print out the col name and number to verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96554fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6682d7",
   "metadata": {},
   "source": [
    "# 2. Plotting the Data with and without NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_nan.groupby('WELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f455771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the labels we want to display on the x-axis\n",
    "#labels = ['RMED','RDEP', 'RHOB','GR', 'NPHI', 'DTC'] # 6 features\n",
    "\n",
    "#labels = ['CALI','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF','DTC', 'SP','DTS','DRHO', 'RMIC','RXO'] # 13 features\n",
    "labels = ['RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "#Setup the figure and the subplots\n",
    "fig, axs = plt.subplots(3, 4, figsize=(20,10))\n",
    "\n",
    "#Loop through each well and column in the grouped dataframe\n",
    "for (name, df), ax in zip(grouped, axs.flat):\n",
    "    #ax.set_xlim(0,5) # 6 features\n",
    "    ax.set_xlim(0,6) # 9 features\n",
    "    \n",
    "    #Setup the depth range\n",
    "    ax.set_ylim(4000, 0)\n",
    "    \n",
    "    #Create multiple fill betweens for each curve# This is between\n",
    "    # the number representing null values and the number representing\n",
    "    # actual values\n",
    "    \n",
    "    #ax.fill_betweenx(df.DEPTH_MD, 0, df.CALI, facecolor='grey')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 0, df.RMED, facecolor='lightgrey')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 1, df.RDEP, facecolor='mediumseagreen')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 2, df.RHOB, facecolor='lightblue')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 3, df.GR, facecolor='lightcoral')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 4, df.NPHI, facecolor='violet')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 5, df.DTC, facecolor='darksalmon')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 6, df.PEF, facecolor='red')\n",
    "    #ax.fill_betweenx(df.DEPTH_MD, 6, df.SP, facecolor='thistle')\n",
    "  \n",
    "    \n",
    "    #Setup the grid, axis labels and ticks\n",
    "    ax.grid(axis='x', alpha=0.5, color='black')\n",
    "    ax.set_ylabel('DEPTH (m)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    #Position vertical lines at the boundaries between the bars\n",
    "    ax.set_xticks([1,2,3,4,5,6,7], minor=False)\n",
    "    \n",
    "    #Position the curve names in the centre of each column\n",
    "    ax.set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5], minor=True)\n",
    "    \n",
    "    #Setup the x-axis tick labels\n",
    "    ax.set_xticklabels(labels,  rotation='vertical', minor=True, verticalalignment='bottom')\n",
    "    ax.set_xticklabels('', minor=False)\n",
    "    ax.tick_params(axis='x', which='minor', pad=-7)\n",
    "    \n",
    "    #Assign the well name as the title to each subplot\n",
    "    ax.set_title(name, fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.savefig('missingdata_northsea.pdf')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4cd02",
   "metadata": {},
   "source": [
    "# 3. Select the headers to use in the in-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aabda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'FACIES'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55db76c",
   "metadata": {},
   "source": [
    "# 4. Column Remapping / Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16818e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_numbers = {30000: 'Sandstone', # sandybrown\n",
    "                     65030: 'Sandstone/Shale', #darkgoldenrod\n",
    "                     65000: 'Shale', # olive\n",
    "                     80000: 'Marl', #gainsboro\n",
    "                     74000: 'Dolomite',\n",
    "                     70000: 'Limestone',\n",
    "                     70032: 'Chalk',\n",
    "                     88000: 'Halite',\n",
    "                     86000: 'Anhydrite',\n",
    "                     99000: 'Tuff',\n",
    "                     90000: 'Coal',\n",
    "                     93000: 'Basement'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3945815",
   "metadata": {},
   "source": [
    "second dictionary to tranform in integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275513ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lithology_numbers = {30000: 1,\n",
    "                            65030: 2,\n",
    "                            65000: 3,\n",
    "                            80000: 4,\n",
    "                            74000: 5,\n",
    "                            70000: 6,\n",
    "                            70032: 7,\n",
    "                            88000: 8,\n",
    "                            86000: 9,\n",
    "                            99000: 10,\n",
    "                            90000: 11,\n",
    "                            93000: 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e06db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['LITH'] = training_data['FACIES'].map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb272051",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['LITH_SI'] = training_data['FACIES'].map(simple_lithology_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e964b7d",
   "metadata": {},
   "source": [
    "# 5. View the number of samples of the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24464aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "training_data['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(training_data['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,11,1)\n",
    "plt.title('Number of samples')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Dolomite',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Halite',\n",
    "                  'Anhydrite',\n",
    "                  'Tuff',\n",
    "                  'Coal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a4bf9",
   "metadata": {},
   "source": [
    "# 6. Crossplot RHOB and NPHI (whole data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce874c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.FacetGrid(training_data, col='LITH', col_wrap=4)\n",
    "g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)\n",
    "g.set(xlim=(-0.15, 1))\n",
    "g.set(ylim=(3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23107df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN\n",
    "training_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd218ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for well in training_data['WELL'].unique():\n",
    "    \n",
    "    print(well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57943888",
   "metadata": {},
   "source": [
    "# 7. sorting out the blind test well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38febc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = training_data[training_data['WELL'] == '16/2-16'] #seleciona um poço apenas do dado\n",
    "training_data = training_data[training_data['WELL'] != '16/2-16'] #remove o poço do dado\n",
    "blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['WELL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.FacetGrid(training_data, col='LITH', col_wrap=4)\n",
    "g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)\n",
    "g.set(xlim=(-0.15, 1))\n",
    "g.set(ylim=(3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d45614",
   "metadata": {},
   "source": [
    "Two lithofacoes are exluded from data after dropping NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "blind['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(blind['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,7,1)\n",
    "plt.title('Samples - Blind well')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Limestone',\n",
    "                  'Anhydrit','Tuff'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62404bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['WELL', 'DEPTH_MD', 'RDEP', 'RHOB','GR', 'NPHI', 'PEF', 'DTC','SP']\n",
    "#col_list = ['LITH_SI','RDEP', 'RHOB','GR', 'NPHI', 'PEF', 'DTC','SP']\n",
    "\n",
    "col_list = ['LITH_SI','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "i=0\n",
    "for col in col_list:\n",
    "    i+=1\n",
    "    plt.subplot(3,4,i)\n",
    "    plt.hist(training_data[col])\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "training_data['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(training_data['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,10,1)\n",
    "plt.title('Samples - Training wells')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Dolomite',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Halite',\n",
    "                  'Tuff',\n",
    "                  'Coal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2993d2e",
   "metadata": {},
   "source": [
    "# 8. Prepare data for modeling and blind test well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29480bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "\n",
    "y = training_data['LITH_SI']\n",
    "X = training_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ab811",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data for modelling\n",
    "\n",
    "#scaler = StandardScaler().fit(X)\n",
    "#X_stnd = scaler.transform(X)\n",
    "\n",
    "# standarization of data for SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f55e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eca1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blind test well\n",
    "\n",
    "y_blind = blind['LITH_SI']\n",
    "X_blind = blind[features]\n",
    "X_blind_stnd = sc.transform(X_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e16f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss and accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1000$]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mae']), label='Train')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mae']),label = 'Val')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,max(history.history['val_mae'])])\n",
    "\n",
    "def plot_prediction(test_labels, test_predictions):\n",
    "    plt.figure()\n",
    "    plt.scatter(test_labels, test_predictions)\n",
    "    plt.xlabel('True Values [1000$]')\n",
    "    plt.ylabel('Predictions [1000$]')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(plt.xlim())\n",
    "    plt.ylim(plt.ylim())\n",
    "    _ = plt.plot([-100, 100],[-100,100])\n",
    "\n",
    "    plt.figure()\n",
    "    error = test_predictions - test_labels\n",
    "    plt.hist(error, bins = 50)\n",
    "    plt.xlabel(\"Prediction Error [1000$]\")\n",
    "    _ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greys):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "\n",
    "    if normalize:\n",
    "        \n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb3962",
   "metadata": {},
   "source": [
    "# 9. Parameter optimization and classifier training\n",
    "\n",
    "Modeling algorithms:\n",
    "1. SVM\n",
    "2. Gradient boosting\n",
    "3. Random forest\n",
    "4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # To use Support Vector Machine\n",
    "from sklearn import ensemble # To use Gradient Boosting and Random forest\n",
    "from sklearn.neighbors import KNeighborsClassifier # To use KNN\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a9dc5",
   "metadata": {},
   "source": [
    "\n",
    "### 9.1 SVM: Parameter optimiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c_list = [0.01, 1, 5, 10, 20, 50, 100, 1000, 5000, 10000]\n",
    "# gamma_list = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "# #gamma_list = [0.0001]\n",
    "# i = 0\n",
    "# plt.figure(figsize=(15,10))\n",
    "\n",
    "# for gamma_value in gamma_list:\n",
    "#     i = i + 1\n",
    "#     scores = list()\n",
    "#     score_stds = list()\n",
    "#     score_tests = list()\n",
    "#     #print('interations gamma_list =',i)\n",
    "#     j = 0\n",
    "#     for c_value in c_list:\n",
    "        \n",
    "#         j = j + 1\n",
    "#         print('interations c_list =',j)\n",
    "        \n",
    "#         clf_cv = SVC(C=c_value, gamma=gamma_value)\n",
    "        \n",
    "#         cv_score = cross_val_score(clf_cv, X_train, y_train)\n",
    "        \n",
    "#         scores.append(np.mean(cv_score))\n",
    "#         score_stds.append(np.std(cv_score))\n",
    "#         clf_cv.fit(X_train, y_train)\n",
    "        \n",
    "#         score_test = clf_cv.score(X_test, y_test)\n",
    "#         score_tests.append(score_test)\n",
    "    \n",
    "#     plt.subplot(2,3,i)\n",
    "#     plt.semilogx(c_list, scores, label='Train error')\n",
    "#     plt.semilogx(c_list, score_tests, label='Cross-validation error')\n",
    "#     #plt.semilogx(c_list, np.array(scores)+np.array(score_stds), 'b--')\n",
    "#     #plt.semilogx(c_list, np.array(scores)+-np.array(score_stds), 'b--')\n",
    "#     plt.title('Gamma = {}'.format(gamma_value))\n",
    "#     plt.xlabel('C values')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.ylim(0,1.1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4041423",
   "metadata": {},
   "source": [
    "SVM classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=10, gamma=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45adf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                  'D',\n",
    "                  'L',\n",
    "                  'Ch',\n",
    "                  'H',\n",
    "                  'T',\n",
    "                  'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97111f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blind_full = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                  'L',\n",
    "                  'Ch',\n",
    "                  'A',\n",
    "                  'T',\n",
    "                  'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893aa5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_test = clf.predict(X_test)\n",
    "print(classification_report(y_test, pred_test))\n",
    "cm_test_SVM = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm_test_SVM, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67385c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_SVM = f1_score(y_test, pred_test, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e659fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind = clf.predict(X_blind_stnd)\n",
    "print(classification_report(y_blind, pred_blind, target_names=list_blind_full,zero_division=0))\n",
    "cm_SVM = confusion_matrix(y_blind, pred_blind)\n",
    "plot_confusion_matrix(cm_SVM, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f047e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_well in range(0,10):\n",
    "    \n",
    "#     aux_pred_svm = clf.predict(X_blind_stnd[i_well])\n",
    "#     microF1_blind_SVM = f1_score(y_blind[i_well], aux_pred_svm, average='micro')\n",
    "#     print('Blind micro f1 score:', microF1_blind_SVM)\n",
    "aux_pred_svm = clf.predict(X_blind_stnd)\n",
    "microF1_blind_SVM = f1_score(y_blind, aux_pred_svm, average='micro')\n",
    "print('Blind micro f1 score:', microF1_blind_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b53ac",
   "metadata": {},
   "source": [
    "### 9.2 Gradient boosting (GB): Parameter optimiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why do not fit and transform  GRADIENT BOOST\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_list = ['deviance']\n",
    "# max_depths = [2,3,4,5,6]\n",
    "\n",
    "# i = 0\n",
    "# plt.figure(figsize=(15,5))\n",
    "\n",
    "\n",
    "# for los in loss_list:\n",
    "    \n",
    "#     i = i + 1\n",
    "#     scores = list()\n",
    "#     score_tests = list()\n",
    "\n",
    "#     for depth in max_depths:\n",
    "        \n",
    "#         params = {'loss': los, ##  loss{‘deviance’, ‘exponential’}, default=’deviance’\n",
    "#                   'learning_rate': 0.1, ##  learning_ratefloat, default=0.1\n",
    "#                   'n_estimators': 500, ##  number of iterations, int, default=100\n",
    "#                   'max_depth': depth, ##  int, default=3\n",
    "#                   'subsample': 1, ## float, default=1.0\n",
    "#                   'min_samples_split': 2 ## int or float, default=2\n",
    "#                  }\n",
    "#         clf_cv = ensemble.GradientBoostingClassifier(**params)\n",
    "        \n",
    "#         # Train data\n",
    "#         clf_cv.fit(X1_train, y1_train)\n",
    "#         cv_score = clf_cv.score(X1_train, y1_train)\n",
    "#         scores.append(np.mean(cv_score))\n",
    "        \n",
    "#         # Test data\n",
    "#         score_test = clf_cv.score(X1_test, y1_test)\n",
    "#         score_tests.append(score_test)\n",
    "    \n",
    "#     plt.subplot(1,2,i)\n",
    "#     plt.plot(max_depths, scores, 'o-', color='b', label='Train')\n",
    "#     plt.plot(max_depths, score_tests, 'o-', color='r', label='Test')\n",
    "#     plt.legend()\n",
    "#     plt.title('Loss = {}'.format(los))\n",
    "#     plt.xlabel('Max depth')\n",
    "#     plt.ylabel('Accuracy')\n",
    "    \n",
    "    \n",
    "#     #ax.semilogx(C_range, cv_errors, label='CV error')\n",
    "#     #ax.semilogx(C_range, train_errors, label='Train error')\n",
    "    \n",
    "#     plt.ylim(0,1.1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce417556",
   "metadata": {},
   "source": [
    "How you could see, there is convergen with 4 deepths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.001, 0.01, 0.1, 0.2, 0.4]\n",
    "# n_estimators_list = [100, 500, 1000]\n",
    "\n",
    "# i = 0\n",
    "# plt.figure(figsize=(15,5))\n",
    "\n",
    "# for est in n_estimators_list:\n",
    "#     i = i + 1\n",
    "#     scores = list()\n",
    "#     score_tests = list()\n",
    "    \n",
    "#     for rate in learning_rates:\n",
    "#         params = {'loss': 'deviance', ##  loss{‘deviance’, ‘exponential’}, default=’deviance’\n",
    "#                   'learning_rate': rate, ##  learning_ratefloat, default=0.1\n",
    "#                   'n_estimators': est, ##  number of iterations, int, default=100\n",
    "#                   'max_depth': 4, ##  int, default=3\n",
    "#                   'subsample': 1, ## float, default=1.0\n",
    "#                   'min_samples_split': 2 ## int or float, default=2\n",
    "#                   }\n",
    "#         clf_cv = ensemble.GradientBoostingClassifier(**params)\n",
    "#         clf_cv.fit(X1_train, y1_train)\n",
    "#         cv_score = clf_cv.score(X1_train, y1_train)\n",
    "#         scores.append(np.mean(cv_score))\n",
    "#         score_test = clf_cv.score(X1_test, y1_test)\n",
    "#         score_tests.append(score_test)\n",
    "        \n",
    "#     plt.subplot(1,3,i)\n",
    "#     plt.semilogx(learning_rates, scores, 'o-', color='b', label='Train')\n",
    "#     plt.semilogx(learning_rates, score_tests, 'o-', color='r', label='Test')\n",
    "#     plt.legend()\n",
    "#     plt.title('N estimators = {}'.format(est))\n",
    "#     plt.xlabel('learning rate')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.ylim(0,1.1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574cc20",
   "metadata": {},
   "source": [
    "N_estimators =500 and learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d602d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsamples = [0.2, 0.6, 1]\n",
    "# n_estimators_list = [500, 1000, 2000]\n",
    "\n",
    "# i = 0\n",
    "# plt.figure(figsize=(15,5))\n",
    "\n",
    "# for est in n_estimators_list:\n",
    "    \n",
    "#     i = i + 1\n",
    "#     scores = list()\n",
    "#     score_tests = list()\n",
    "    \n",
    "#     for sub in subsamples:\n",
    "        \n",
    "#         params = {'loss': 'deviance', ##  loss{‘deviance’, ‘exponential’}, default=’deviance’\n",
    "#                   'learning_rate': 0.1, ##  learning_ratefloat, default=0.1\n",
    "#                   'n_estimators': est, ##  number of iterations, int, default=100\n",
    "#                   'max_depth': 5, ##  int, default=3\n",
    "#                   'subsample': sub, ## float, default=1.0\n",
    "#                   'min_samples_split': 2 ## int or float, default=2\n",
    "#                   }\n",
    "        \n",
    "#         clf_cv = ensemble.GradientBoostingClassifier(**params)\n",
    "#         clf_cv.fit(X_train, y_train)\n",
    "#         cv_score = clf_cv.score(X_train, y_train)\n",
    "        \n",
    "#         scores.append(np.mean(cv_score))\n",
    "#         score_test = clf_cv.score(X_test, y_test)\n",
    "#         score_tests.append(score_test)\n",
    "        \n",
    "        \n",
    "#     plt.subplot(1,3,i)\n",
    "#     plt.plot(subsamples, scores, 'o-', color='b', label='Train')\n",
    "#     plt.plot(subsamples, score_tests, 'o-', color='r', label='Test')\n",
    "#     plt.legend()\n",
    "#     plt.title('n_estimators = {}'.format(est))\n",
    "#     plt.xlabel('sub samples')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.ylim(0,1.1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81451526",
   "metadata": {},
   "source": [
    "Based on the accuracy plot, max_depth=4, learning_rate=0.1, n_estimators=500, subsample=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c95490",
   "metadata": {},
   "source": [
    "Gradient Boosting classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868bac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_list_gb = ['Sandstone',\n",
    " #              'Sandstone/Shale',\n",
    "  #             'Shale','Marl',\n",
    "   #            'Dolomite',\n",
    "    #           'Limestone',\n",
    "     #          'Chalk',\n",
    "      #         'Tuff']\n",
    "\n",
    "params = {#'loss': 'deviance', ##  loss{‘deviance’, ‘exponential’}, default=’deviance’\n",
    "          'learning_rate': 0.1, ##  learning_ratefloat, default=0.1\n",
    "          'n_estimators': 500, ##  number of iterations, int, default=100\n",
    "          'max_depth': 3, ##  int, default=3\n",
    "          'subsample': 1, ## float, default=1.0\n",
    "          'min_samples_split': 2 ## int or float, default=2\n",
    "          }\n",
    "clf_GB = ensemble.GradientBoostingClassifier(**params)\n",
    "clf_GB.fit(X1_train, y1_train)\n",
    "preds_GB = clf_GB.predict(X1_test)\n",
    "\n",
    "print(classification_report(y1_test, preds_GB))\n",
    "cm_test_GB = confusion_matrix(y1_test, preds_GB)\n",
    "plot_confusion_matrix(cm_test_GB, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_gb = f1_score(y1_test, preds_GB, average='micro')\n",
    "print('Test Micro f1 score:', microF1_test_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blind_full = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                   'D',\n",
    "                  'L',\n",
    "                  'Ch',\n",
    "                  'A',\n",
    "                  'T',\n",
    "                  'C']\n",
    "\n",
    "pred_GB_blind = clf_GB.predict(X_blind)\n",
    "print(classification_report(y_blind, pred_GB_blind))\n",
    "cm_GB = confusion_matrix(y_blind, pred_GB_blind)\n",
    "plot_confusion_matrix(cm_GB, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f81e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_well in range(0,10):\n",
    "    \n",
    "\n",
    "#     aux_pred_GB_blinda = clf_GB.predict(X_blind[i_well])\n",
    "\n",
    "#     microF1_blind_GB = f1_score(y_blind[i_well], aux_pred_GB_blinda, average='micro')\n",
    "    \n",
    "#     print('Blind micro f1 score:', microF1_blind_GB)\n",
    "microF1_blind_GB = f1_score(y_blind, pred_GB_blind, average='micro')    \n",
    "print('Blind micro f1 score:', microF1_blind_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1685dac",
   "metadata": {},
   "source": [
    "### 9.3. Random forest (RF) parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc28654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depths = [2, 3, 4]\n",
    "# n_estimators_list = [100, 500, 1000, 2000, 5000]\n",
    "\n",
    "# i = 0\n",
    "# plt.figure(figsize=(15,5))\n",
    "\n",
    "# for depth in max_depths:\n",
    "    \n",
    "#     i = i + 1\n",
    "#     scores = list()\n",
    "#     score_tests = list()\n",
    "    \n",
    "#     for est in n_estimators_list:\n",
    "#         params = {'n_estimators': est, ##  number of iterations, int, default=100\n",
    "#                   'max_depth': depth, ##  int, default=None\n",
    "#                   'n_jobs': -1 #to speed up computations by taking advantage of parallel processing.\n",
    "                  \n",
    "#                   }\n",
    "#         clf_cv = ensemble.RandomForestClassifier(**params)\n",
    "#         clf_cv.fit(X1_train, y1_train)\n",
    "#         cv_score = clf_cv.score(X1_train, y1_train)\n",
    "#         scores.append(np.mean(cv_score))\n",
    "#         score_test = clf_cv.score(X1_test, y1_test)\n",
    "#         score_tests.append(score_test)\n",
    "        \n",
    "#     plt.subplot(1,4,i)\n",
    "#     plt.plot(n_estimators_list, scores, color='b', label='Train')\n",
    "#     plt.plot(n_estimators_list, score_tests, color='r', label='Test')\n",
    "#     plt.legend()\n",
    "#     plt.title('max depth = {}'.format(depth))\n",
    "#     plt.xlabel('n_estimators')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.ylim(0,1.1)\n",
    "\n",
    "# scores = list()\n",
    "# score_tests = list()\n",
    "\n",
    "# for est in n_estimators_list:\n",
    "    \n",
    "#     clf_cv = ensemble.RandomForestClassifier(n_estimators=est)\n",
    "#     clf_cv.fit(X1_train, y1_train)\n",
    "#     cv_score = clf_cv.score(X1_train, y1_train)\n",
    "#     scores.append(np.mean(cv_score))\n",
    "#     score_test = clf_cv.score(X1_test, y1_test)\n",
    "#     score_tests.append(score_test)\n",
    "    \n",
    "# plt.subplot(1,4,4)\n",
    "# plt.plot(n_estimators_list, scores, color='b', label='Train')\n",
    "# plt.plot(n_estimators_list, score_tests, color='r', label='Test')\n",
    "# plt.legend()\n",
    "# plt.title('max depth = {}'.format('None'))\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim(0,1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d773fb",
   "metadata": {},
   "source": [
    "Max_depth can be default (None), and n_estimator = 2000 gives best accuracy.\n",
    "\n",
    "Random forest classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49397da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = ensemble.RandomForestClassifier(n_estimators=2000, criterion='gini')\n",
    "#print(cross_val_score(clf_RF, X1_train, y1_train, cv=5))\n",
    "clf_RF.fit(X1_train, y1_train)\n",
    "preds_RF = clf_RF.predict(X1_test)\n",
    "print(classification_report(y1_test, preds_RF))\n",
    "cm_test_RF = confusion_matrix(y1_test, preds_RF)\n",
    "plot_confusion_matrix(cm_test_RF, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fe723",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_rf = f1_score(y1_test, preds_RF, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e7332",
   "metadata": {},
   "source": [
    "Random forest blind predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blind_full = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                   'D',\n",
    "                  'L',\n",
    "                  'Ch',\n",
    "                  'A',\n",
    "                  'T']\n",
    "\n",
    "preds_RF_blind = clf_RF.predict(X_blind)\n",
    "print(classification_report(y_blind, preds_RF_blind))\n",
    "cm_RF = confusion_matrix(y_blind, preds_RF_blind)\n",
    "plot_confusion_matrix(cm_RF, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71887092",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_blind_rf = f1_score(y_blind, preds_RF_blind, average='micro')\n",
    "print('Test Micro f1 score:', microF1_blind_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805e6dc",
   "metadata": {},
   "source": [
    "### 9.4. KNN Parameter optimzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703afe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbor_list = [5, 7, 10]\n",
    "# weight_list = ['uniform', 'distance']\n",
    "# i=0\n",
    "\n",
    "# for weight in weight_list:\n",
    "    \n",
    "    \n",
    "#     scores = list()\n",
    "#     score_tests = list()\n",
    "#     i = i + 1\n",
    "    \n",
    "#     for neighbor in neighbor_list:\n",
    "#         clf_cv = KNeighborsClassifier(n_neighbors=neighbor, weights=weight)\n",
    "#         clf_cv.fit(X1_train, y1_train)\n",
    "#         scores.append(clf_cv.score(X1_train, y1_train))\n",
    "#         score_tests.append(clf_cv.score(X1_test, y1_test))\n",
    "#         print(scores)\n",
    "        \n",
    "#     plt.subplot(1,3,i)\n",
    "#     plt.plot(neighbor_list, scores, 'b')\n",
    "#     plt.plot(neighbor_list, score_tests, 'r')\n",
    "#     plt.title('Weight = {}'.format(weight))\n",
    "#     plt.xlabel('Number of neighbors')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.ylim(0,1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b4a51",
   "metadata": {},
   "source": [
    "Using weight has a better KNN modeling score.\n",
    "\n",
    "KNN classifer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca79411",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = KNeighborsClassifier(weights='distance')\n",
    "print(cross_val_score(clf_knn, X1_train, y1_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = KNeighborsClassifier(weights='distance')\n",
    "clf_knn.fit(X1_train, y1_train)\n",
    "preds_knn = clf_knn.predict(X1_test)\n",
    "\n",
    "print(classification_report(y1_test, preds_knn))\n",
    "cm_test_knn = confusion_matrix(y1_test, preds_knn)\n",
    "plot_confusion_matrix(cm_test_knn, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a695b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_knn = f1_score(y1_test, preds_knn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e24cc",
   "metadata": {},
   "source": [
    "KNN blind well prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c355c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blind_rf = ['Ss',\n",
    "              'Ss/Sh',\n",
    "              'Sh',\n",
    "              'M',\n",
    "                 'D',\n",
    "                 'L',\n",
    "                 'Ch',\n",
    "                 'A','T','C']\n",
    "\n",
    "\n",
    "preds_knn_blind = clf_knn.predict(X_blind)\n",
    "print(classification_report(y_blind, preds_knn_blind))\n",
    "cm_knn = confusion_matrix(y_blind, preds_knn_blind)\n",
    "plot_confusion_matrix(cm_knn, list_blind_rf, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c34ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_blind_knn = f1_score(y_blind, preds_knn_blind, average='micro')\n",
    "print('Test Macro f1 score:', microF1_blind_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8b48b",
   "metadata": {},
   "source": [
    "### 9.5 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[1].shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ae6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_train.shape[0] # number of samples in train set\n",
    "time_steps  = X_train.shape[1] # number of features in train set\n",
    "input_dimension = 1               # each feature is represented by 1 number\n",
    "\n",
    "train_data_reshaped = X_train.reshape(sample_size,time_steps,input_dimension)\n",
    "print(\"After reshape train data set shape:\\n\", train_data_reshaped.shape)\n",
    "print(\"1 Sample shape:\\n\",train_data_reshaped[0].shape)\n",
    "print(\"An example sample:\\n\", train_data_reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1D_model():\n",
    "\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "       \n",
    "    \n",
    "    model = keras.Sequential(name=\"model_conv1D\")\n",
    "    \n",
    "    # 1st layer\n",
    "    ks = 2\n",
    "    model.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_1\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_2\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_3\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_4\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    \n",
    "    #model.add(keras.layers.MaxPooling1D(pool_size=1, name=\"MaxPooling1D_fisrt\"))\n",
    "    \n",
    "    # Dense\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_conv1D = build_conv1D_model()\n",
    "model_conv1D.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84baee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystoping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                patience=5,\n",
    "                                                verbose=1,\n",
    "                                                mode='auto',\n",
    "                                                restore_best_weights=True)\n",
    "checkpoint_filepath = 'weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      monitor='val_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model_conv1D.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'])\n",
    "plt.plot(history_cnn.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cnn = model_conv1D.predict(test_data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cnn = tf.argmax(pred_test_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_conv1D.evaluate(test_data_reshaped,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test_cnn, target_names=training_features))\n",
    "cm_test_cnn = confusion_matrix(y_test, pred_test_cnn)\n",
    "plot_confusion_matrix(cm_test_cnn, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd138124",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_cnn = f1_score(y_test, pred_test_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blind_reshaped = X_blind_stnd.reshape(X_blind_stnd.shape[0],X_blind_stnd.shape[1],1)\n",
    "X_blind_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = model_conv1D.predict(X_blind_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_cnn = tf.argmax(aux, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2031d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_blind, pred_blind_cnn))\n",
    "cm_cnn = confusion_matrix(y_blind, pred_blind_cnn)\n",
    "plot_confusion_matrix(cm_cnn, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_blind_cnn = f1_score(y_blind, pred_blind_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_blind_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0f4d3",
   "metadata": {},
   "source": [
    "### 9.6 CNN (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Layer\n",
    "# from keras import backend as K\n",
    "\n",
    "# class RBFLayer(Layer):\n",
    "#     def __init__(self, units, gamma, ** kwargs):\n",
    "#         super(RBFLayer, self).__init__( ** kwargs)\n",
    "#         self.units = units\n",
    "#         self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.mu = self.add_weight(name = 'mu',\n",
    "#                                   shape = (int(input_shape[1]), self.units),\n",
    "#                                   initializer = 'uniform',\n",
    "#                                   trainable = True)\n",
    "#         super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         diff = K.expand_dims(inputs) - self.mu\n",
    "#         l2 = K.sum(K.pow(diff, 2), axis = 1)\n",
    "#         res = K.exp(-1 * self.gamma * l2)\n",
    "#         return res\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], self.units)\n",
    "from rbflayer import RBFLayer, InitCentersRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1D_rbf_model():\n",
    "    #\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "    #\n",
    "    model_rbf = keras.Sequential(name=\"model_conv1D_rbf\")\n",
    "    # 1st layer\n",
    "    ks = 2\n",
    "    mp=1\n",
    "    f=128\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_1\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_2\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 2nd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_3\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_4\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 3rd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_5\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_6\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # Dense\n",
    "    model_rbf.add(keras.layers.Flatten())\n",
    "    model_rbf.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model_rbf.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model_rbf\n",
    "\n",
    "model_conv1D_rbf = build_conv1D_rbf_model()\n",
    "model_conv1D_rbf.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rbf = model_conv1D_rbf.fit(\n",
    "    train_data_reshaped, \n",
    "    y_train, \n",
    "    epochs = 1000,\n",
    "    steps_per_epoch=len(train_data_reshaped)/10,\n",
    "    validation_data = (test_data_reshaped,y_test),\n",
    "    validation_steps= len(test_data_reshaped),\n",
    "    batch_size = 512, \n",
    "    callbacks = [model_checkpoint,earlystoping], \n",
    "    \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['loss'])\n",
    "plt.plot(history_rbf.history['val_loss'])\n",
    "plt.title('model loss CNN (RBF)')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f913bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['accuracy'])\n",
    "plt.plot(history_rbf.history['val_accuracy'])\n",
    "plt.title('model accuracy CNN (RBF)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee927129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rbf = model_conv1D_rbf.predict(test_data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rbf = tf.argmax(pred_test_rbf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test_rbf))\n",
    "cm_test_rbf = confusion_matrix(y_test, pred_test_rbf)\n",
    "plot_confusion_matrix(cm_test_rbf, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_rbf = f1_score(y_test, pred_test_rbf, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_rbf = model_conv1D_rbf.predict(X_blind_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_rbf = tf.argmax(pred_blind_rbf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_blind = ['Ss',\n",
    "              'Ss/Sh',\n",
    "              'Sh',\n",
    "              'M',\n",
    "              'L',\n",
    "              'T']\n",
    "\n",
    "print(classification_report(y_blind, pred_blind_rbf))\n",
    "cm_rbf = confusion_matrix(y_blind, pred_blind_rbf)\n",
    "plot_confusion_matrix(cm_rbf, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d17342",
   "metadata": {},
   "source": [
    "### 9.7. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30307ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42, hidden_layer_sizes=(50,50)).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_mlp_test, target_names=training_features))\n",
    "cm_test_MLP = confusion_matrix(y_test, pred_mlp_test)\n",
    "plot_confusion_matrix(cm_test_MLP, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_mlp = f1_score(y_test, pred_mlp_test, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp_blind = mlp.predict(X_blind_stnd)\n",
    "print(classification_report(y_blind, pred_mlp_blind))\n",
    "cm_mlp = confusion_matrix(y_blind, pred_mlp_blind)\n",
    "plot_confusion_matrix(cm_mlp, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d64c16",
   "metadata": {},
   "source": [
    "# 10. Model performance evaluation\n",
    "\n",
    "I will use the diagnosis of confusion matrix from train data set to evaluate the model performance. The diagnosis of confusion matrix points how much percentage of the stone is correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of \n",
    "### facies for each machine learning algorithm\n",
    "\n",
    "mod_test_list = ['SVM', 'GB', 'RF','KNN','CNN','CNN-RBF','MLP']\n",
    "cm_test_list = [cm_test_SVM, cm_test_GB, cm_test_RF, cm_test_knn,cm_test_cnn, cm_test_rbf, cm_test_MLP]\n",
    "face_test_list = training_features\n",
    "pred_test_df = pd.DataFrame(index=training_features, columns=mod_test_list)\n",
    "\n",
    "for mod in mod_test_list:\n",
    "    \n",
    "    col_index = int(mod_test_list.index(mod))\n",
    "    cm = cm_test_list[col_index]\n",
    "    \n",
    "    for face in face_test_list:\n",
    "        row_index = training_features.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_test_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "        \n",
    "\n",
    "### add the accuracy factor\n",
    "df_1 = pd.DataFrame([[microF1_test_SVM, \n",
    "                      microF1_test_gb, \n",
    "                      microF1_test_rf, \n",
    "                      microF1_test_knn, \n",
    "                      microF1_test_cnn, \n",
    "                      microF1_test_rbf, \n",
    "                      microF1_test_mlp]], index=['Accuracy'], columns=mod_test_list)    \n",
    "\n",
    "\n",
    "pred_test_conc = pd.concat([pred_test_df,df_1])\n",
    "pred_test_conc\n",
    "\n",
    "### To create a data frame recording the correct prediction (normalized) of \n",
    "### facies for each machine learning algorithm\n",
    "\n",
    "#mod_test_list = ['SVM', 'GB', 'RF','KNN']\n",
    "#cm_test_list = [cm_test_SVM, cm_test_GB, cm_test_RF, cm_test_knn]\n",
    "#face_test_list = ['Sandstone','Sandstone/Shale','Shale','Marl','Dolomite','Limestone','Chalk']\n",
    "#pred_test_df = pd.DataFrame(index=target_list, columns=mod_test_list)\n",
    "\n",
    "#for mod in mod_test_list:\n",
    "#    \n",
    "#    col_index = int(mod_test_list.index(mod))\n",
    "#    cm = cm_test_list[col_index]\n",
    "#    \n",
    "#    for face in face_test_list:\n",
    "#        row_index = target_list.index(face)\n",
    "#        #print(face, row_index, col_index)\n",
    "#        pred_test_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "        \n",
    "\n",
    "### add the accuracy factor\n",
    "#df_1 = pd.DataFrame([[0.94, 0.93, 0.94, 0.93]], index=['Accuracy'], columns=mod_test_list)    \n",
    "#pred_test_df = pred_test_df.append(df_1)\n",
    "\n",
    "#print(pred_test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47632fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_test_df.shape[0])\n",
    "(pred_df_index_list) = training_features\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_test_df['SVM'], color='k', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_test_df['GB'], color='yellow', width=aux)\n",
    "plt.bar(X_ind+0.2, pred_test_df['RF'], color='darkgreen', width=aux)\n",
    "plt.bar(X_ind+0.3, pred_test_df['KNN'], color='orange', width=aux)\n",
    "plt.bar(X_ind+0.4, pred_test_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.5, pred_test_df['CNN-RBF'], color='red', width=aux)\n",
    "plt.bar(X_ind+0.6, pred_test_df['MLP'], color='lime', width=aux)\n",
    "plt.xticks(X_ind, pred_df_index_list)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_test_list)\n",
    "plt.savefig('canada_performance_evaluation_test_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c254c6",
   "metadata": {},
   "source": [
    "# 11. Calssifier evluation using blind test well\n",
    "\n",
    "I will use the same method shown in item4 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of facies of blind test well for each machine learning algorithm\n",
    "\n",
    "blind_class  = ['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Anhydrite',\n",
    "                  'Tuff']\n",
    "\n",
    "mod_list = ['SVM', 'GB', 'RF','KNN','CNN','CNN-RBF','MLP']\n",
    "cm_list = [cm_SVM, cm_GB, cm_RF, cm_knn, cm_cnn, cm_rbf, cm_mlp]\n",
    "pred_df = pd.DataFrame(index=blind_class, columns=mod_list)\n",
    "\n",
    "for mod in mod_list:\n",
    "    col_index = int(mod_list.index(mod))\n",
    "    cm = cm_list[col_index]\n",
    "    \n",
    "    for face in blind_class:\n",
    "        \n",
    "        row_index = blind_class.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_knn_blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1affc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_df.shape[0])\n",
    "\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_df['SVM'], color='k', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_df['GB'], color='yellow', width=aux)\n",
    "plt.bar(X_ind+0.2, pred_df['RF'], color='darkgreen', width=aux)\n",
    "plt.bar(X_ind+0.3, pred_df['KNN'], color='orange', width=aux)\n",
    "plt.bar(X_ind+0.4, pred_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.5, pred_df['CNN-RBF'], color='red', width=aux)\n",
    "plt.bar(X_ind+0.6, pred_df['MLP'], color='lime', width=aux)\n",
    "plt.xticks(X_ind, blind_class)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_list)\n",
    "plt.savefig('canada_performance_evaluation_blind_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe72fa",
   "metadata": {},
   "source": [
    "# 12. Plot the predicted facies for comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = blind.copy()\n",
    "blind['SVM'] = pred_blind\n",
    "blind['GB'] = pred_GB_blind\n",
    "blind['RF'] = preds_RF_blind\n",
    "blind['KNN'] = preds_knn_blind\n",
    "blind['CNN'] = pred_blind_cnn\n",
    "blind['RBF'] = pred_blind_rbf\n",
    "blind['MLP'] = pred_mlp_blind\n",
    "\n",
    "blind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "facies_colors = ['bisque',\n",
    "                 'darkorange',\n",
    "                 'darkgoldenrod',\n",
    "                 'peachpuff',\n",
    "                 'beige',\n",
    "                 'honeydew',\n",
    "                 'white','red']\n",
    "\n",
    "blind_class  = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                  'L',\n",
    "                  'Ch',\n",
    "                  'A',\n",
    "                  'T']\n",
    "\n",
    "def compare_facies_plot(logs, compare1, compare2, compare3, compare4, compare5, compare6, compare7, facies_colors):\n",
    "      #make sure logs are sorted by depth\n",
    "    logs = logs.sort_values(by='DEPTH_MD')\n",
    "    cmap_facies = colors.ListedColormap(\n",
    "            facies_colors[0:len(facies_colors)], 'indexed')\n",
    "    num_colors = 8\n",
    "    ztop=logs.DEPTH_MD.min(); zbot=logs.DEPTH_MD.max()\n",
    "    \n",
    "    cluster0 = np.repeat(np.expand_dims(logs['LITH_SI'].values,1), 100, 1)\n",
    "    cluster1 = np.repeat(np.expand_dims(logs[compare1].values,1), 100, 1)\n",
    "    cluster2 = np.repeat(np.expand_dims(logs[compare2].values,1), 100, 1)\n",
    "    cluster3 = np.repeat(np.expand_dims(logs[compare3].values,1), 100, 1)\n",
    "    cluster4 = np.repeat(np.expand_dims(logs[compare4].values,1), 100, 1)\n",
    "    cluster5 = np.repeat(np.expand_dims(logs[compare5].values,1), 100, 1)\n",
    "    cluster6 = np.repeat(np.expand_dims(logs[compare6].values,1), 100, 1)\n",
    "    cluster7 = np.repeat(np.expand_dims(logs[compare7].values,1), 100, 1)\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=15, figsize=(18, 15))\n",
    "    ax[0].plot(logs.RMED, logs.DEPTH_MD, '-g')\n",
    "    ax[1].plot(logs.RDEP, logs.DEPTH_MD, '-')\n",
    "    ax[2].plot(logs.RHOB, logs.DEPTH_MD, '-', color='0.5')\n",
    "    ax[3].plot(logs.GR, logs.DEPTH_MD, '-', color='r')\n",
    "    ax[4].plot(logs.NPHI, logs.DEPTH_MD, '-', color='black')\n",
    "    ax[5].plot(logs.DTC, logs.DEPTH_MD, '-', color='black')\n",
    "    ax[6].plot(logs.PEF, logs.DEPTH_MD, '-', color='black')\n",
    "    im0 = ax[7].imshow(cluster0, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im1 = ax[8].imshow(cluster1, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im2 = ax[9].imshow(cluster2, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im3 = ax[10].imshow(cluster3, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im4 = ax[11].imshow(cluster4, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im4 = ax[12].imshow(cluster5, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im4 = ax[13].imshow(cluster6, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im4 = ax[14].imshow(cluster7, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    \n",
    "            \n",
    "    divider = make_axes_locatable(ax[14])\n",
    "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
    "    cbar=plt.colorbar(im4, cax=cax)\n",
    "    cbar.set_label((30*' ').join(blind_class))\n",
    "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
    "    \n",
    "    for i in range(len(ax)-8):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=3)\n",
    "    \n",
    "    ax[0].set_xlabel(\"RMED\")\n",
    "    ax[0].set_xlim(logs.RMED.min(),logs.RMED.max())\n",
    "    \n",
    "    ax[1].set_xlabel(\"RDEP\")\n",
    "    ax[1].set_xlim(logs.RDEP.min(),logs.RDEP.max())\n",
    "    \n",
    "    ax[2].set_xlabel(\"RHOB\")\n",
    "    ax[2].set_xlim(logs.RHOB.min(),logs.RHOB.max())\n",
    "    \n",
    "    ax[3].set_xlabel(\"GR\")\n",
    "    ax[3].set_xlim(logs.GR.min(),logs.GR.max())\n",
    "    \n",
    "    ax[4].set_xlabel(\"NPHI\")\n",
    "    ax[4].set_xlim(logs.NPHI.min(),logs.NPHI.max())\n",
    "    \n",
    "    ax[5].set_xlabel(\"DTC\")\n",
    "    ax[5].set_xlim(logs.DTC.min(),logs.DTC.max())\n",
    "    \n",
    "    ax[6].set_xlabel(\"PEF\")\n",
    "    ax[6].set_xlim(logs.PEF.min(),logs.PEF.max())\n",
    "    \n",
    "    ax[7].set_xlabel('Facies')\n",
    "    ax[8].set_xlabel(compare1)\n",
    "    ax[9].set_xlabel(compare2)\n",
    "    ax[10].set_xlabel(compare3)\n",
    "    ax[11].set_xlabel(compare4)\n",
    "    ax[12].set_xlabel(compare5)\n",
    "    ax[13].set_xlabel(compare6)\n",
    "    ax[14].set_xlabel(compare7)\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    ax[4].set_yticklabels([]); ax[5].set_yticklabels([]); ax[6].set_yticklabels([])\n",
    "    ax[7].set_yticklabels([]); ax[8].set_yticklabels([]); ax[9].set_yticklabels([])\n",
    "    ax[10].set_yticklabels([]); ax[11].set_yticklabels([]); ax[12].set_yticklabels([])\n",
    "    ax[13].set_yticklabels([]); ax[14].set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    ax[5].set_xticklabels([])\n",
    "    ax[6].set_xticklabels([])\n",
    "    ax[7].set_xticklabels([])\n",
    "    ax[8].set_xticklabels([])\n",
    "    ax[9].set_xticklabels([])\n",
    "    ax[10].set_xticklabels([])\n",
    "    ax[11].set_xticklabels([])\n",
    "    ax[12].set_xticklabels([])\n",
    "    ax[13].set_xticklabels([])\n",
    "    ax[14].set_xticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['WELL'], fontsize=14,y=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_facies_plot(blind, 'SVM', 'GB', 'RF', 'KNN','CNN','RBF','MLP', facies_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "list0 = []\n",
    "for i in range(0,100,1):\n",
    "    X_train0, X_test0, y_train0, y_test0 = train_test_split(X, y, test_size=0.05)\n",
    "    clf0 = ensemble.RandomForestClassifier(n_estimators=2000)\n",
    "    clf0.fit(X_train0, y_train0)\n",
    "    list0.append(clf0.score(X_test0, y_test0))\n",
    "    print(i + 1, clf0.score(X_test0, y_test0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
