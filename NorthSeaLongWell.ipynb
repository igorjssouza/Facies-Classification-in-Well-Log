{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b570a7",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "filename = 'output.csv' # nome do dado de entrada\n",
    "df = pd.read_csv(filename) # leitura do dado de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata \n",
    "training_data = df[['WELL','CALI','DEPTH_MD','GR','LITH','NPHI','RDEP','RHOB','RMED']].copy()\n",
    "lith_num = {30000: 1,65030: 2,65000: 3,80000: 4,74000: 5,70000: 6,70032: 7,88000: 8,86000: 9,99000: 10,90000: 11,93000: 12}\n",
    "training_data['LITH_SI'] = training_data['LITH'].map(lith_num)\n",
    "blind = training_data[training_data['WELL'] == '16/2-16'] #seleciona um poço apenas do dado\n",
    "training_data = training_data[training_data['WELL'] != '16/2-16'] #remove o poço do dado\n",
    "training_data['WELL'].unique()\n",
    "features = ['CALI','GR','NPHI','RDEP','RHOB','RMED']\n",
    "#Select Facies\n",
    "y = training_data['LITH_SI']\n",
    "# well curves\n",
    "X = training_data[features]\n",
    "#split data to train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "# Blind test well\n",
    "y_blind = blind['LITH_SI']\n",
    "X_blind = blind[features]\n",
    "X_blind_stnd = sc.transform(X_blind)\n",
    "training_features = ['Ss','Ss/Sh','Sh','M','D','L','Ch','A','H','T','C']\n",
    "# list_blind_full =   ['Ss','Ss/Sh','Sh','M','L','Ch','A','T','C']\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','NaN','L','NaN','A','T']\n",
    "sample_size = X_train.shape[0] # number of samples in train set\n",
    "time_steps  = X_train.shape[1] # number of features in train set\n",
    "input_dimension = 1               # each feature is represented by 1 number\n",
    "\n",
    "train_data_reshaped = X_train.reshape(sample_size,time_steps,input_dimension)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_encoded = to_categorical(y_train - 1, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test - 1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dca771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dropout, Dense, LeakyReLU, Reshape, Conv1DTranspose, Concatenate, Embedding\n",
    "\n",
    "def build_discriminator(input_shape=(6, 1), num_classes=11):\n",
    "    data_input = Input(shape=input_shape)\n",
    "    label_input = Input(shape=(num_classes,))\n",
    "    labels_reshaped = Reshape((1, num_classes))(label_input)\n",
    "    labels_reshaped = tf.keras.layers.UpSampling1D(size=6)(labels_reshaped)\n",
    "    concatenated = Concatenate(axis=-1)([data_input, labels_reshaped])\n",
    "    \n",
    "    x = Conv1D(64, 2, strides=2, padding='same', activation='relu')(concatenated)\n",
    "    x = Dropout(0.4)(x)  # Dropout added\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model([data_input, label_input], output, name=\"discriminator\")\n",
    "    # Compile the discriminator model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    # Inputs\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    label_input = Input(shape=(num_classes,))\n",
    "    \n",
    "    # Merging noise and label inputs\n",
    "    merged_input = Concatenate()([noise_input, label_input])\n",
    "    \n",
    "    # Intermediate layer\n",
    "    x = Dense(60)(merged_input)  # Adjusted number of units\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    # Branch for well profiles (Assuming well profiles are time-series data)\n",
    "    x_profiles = Reshape((6, 10))(x)\n",
    "    x_profiles = Conv1DTranspose(64, 3, strides=1, padding='same')(x_profiles)\n",
    "    x_profiles = LeakyReLU(alpha=0.2)(x_profiles)\n",
    "    x_profiles = Conv1DTranspose(32, 3, strides=1, padding='same')(x_profiles)\n",
    "    x_profiles = LeakyReLU(alpha=0.2)(x_profiles)\n",
    "    well_profiles_output = Conv1D(1, 3, activation='linear', padding='same')(x_profiles)\n",
    "    well_profiles_output = Reshape((6, 1))(well_profiles_output)\n",
    "    \n",
    "    # Branch for lithology predictions\n",
    "    x_lithology = Dense(num_classes, activation='softmax')(x)  # Softmax for multi-class classification\n",
    "    \n",
    "    return Model([noise_input, label_input], [well_profiles_output, x_lithology], name=\"generator\")\n",
    "    # return Model([noise_input, label_input], output, name=\"generator\")\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    # Make the discriminator not trainable when compiling GAN\n",
    "    discriminator.trainable = False\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    labels = Input(shape=(num_classes,))\n",
    "    gen_data, gen_labels = generator([noise, labels])\n",
    "    valid = discriminator([gen_data, labels])\n",
    "    gan = Model([noise, labels], [valid, gen_labels])\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer=opt)\n",
    "    return gan\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_real_samples(dataset, labels, n_samples):\n",
    "    # Select random instances\n",
    "    idx = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[idx]\n",
    "    y = np.ones((n_samples, 1))  # Real samples label: 1\n",
    "    labels = labels[idx]\n",
    "    return X, labels, y\n",
    "\n",
    "# Function to generate fake samples\n",
    "def generate_fake_samples(generator, latent_dim, num_classes, n_samples):\n",
    "    # Generate points in latent space\n",
    "    noise = np.random.randn(latent_dim * n_samples)\n",
    "    noise = noise.reshape(n_samples, latent_dim)\n",
    "    # Generate random labels\n",
    "    sampled_labels = np.random.randint(0, num_classes, n_samples)\n",
    "    sampled_labels = to_categorical(sampled_labels, num_classes=num_classes)\n",
    "    # Generate fake samples\n",
    "    X, y_labels = generator.predict([noise, sampled_labels])\n",
    "    y = np.zeros((n_samples, 1))  # Fake samples label: 0\n",
    "    return X, y_labels, y\n",
    "\n",
    "# Training the GAN\n",
    "def train_gan(generator, discriminator, gan, dataset, labels, latent_dim, n_epochs=1000, n_batch=128):\n",
    "    half_batch = n_batch // 2\n",
    "    for i in range(n_epochs):\n",
    "        # Get randomly selected 'real' samples and their labels\n",
    "        X_real, labels_real, y_real = get_real_samples(dataset, labels, half_batch)\n",
    "        y_real_labels = labels_real  # Additional real labels for lithology\n",
    "        # Generate 'fake' examples and random labels\n",
    "        X_fake, labels_fake, y_fake = generate_fake_samples(generator, latent_dim, num_classes, half_batch)\n",
    "        y_fake_labels = labels_fake  # Fake labels for lithology\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator.train_on_batch([X_real, labels_real], [y_real, y_real_labels])\n",
    "        discriminator.train_on_batch([X_fake, labels_fake], [y_fake, y_fake_labels])\n",
    "        # Prepare points in latent space as input for the generator\n",
    "        noise = np.random.randn(latent_dim * n_batch)\n",
    "        sampled_labels = np.random.randint(0, num_classes, n_batch)\n",
    "        sampled_labels = to_categorical(sampled_labels, num_classes=num_classes)\n",
    "        noise = noise.reshape(n_batch, latent_dim)\n",
    "        # The generator wants the discriminator to label the generated samples as valid (1)\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        y_gan_labels = sampled_labels  # Generator aims to fool the discriminator with these labels\n",
    "        # Update the generator via the discriminator's error\n",
    "        gan.train_on_batch([noise, sampled_labels], [y_gan, y_gan_labels])\n",
    "        if (i+1) % (n_epochs // 10) == 0:\n",
    "            summarize_performance(i, generator, discriminator, dataset, labels, latent_dim)\n",
    "\n",
    "# Summarize the performance of the discriminator and generator\n",
    "def summarize_performance(epoch, generator, discriminator, dataset, labels, latent_dim, n_samples=100):\n",
    "    X_real, labels_real, y_real = get_real_samples(dataset, labels, n_samples)\n",
    "    acc_real = discriminator.evaluate([X_real, labels_real], y_real, verbose=0)\n",
    "    X_fake, labels_fake, y_fake = generate_fake_samples(generator, latent_dim, num_classes, n_samples)\n",
    "    acc_fake = discriminator.evaluate([X_fake, labels_fake], y_fake, verbose=0)\n",
    "    print(f\"Epoch {epoch+1}, Accuracy Real: {acc_real*100}, Accuracy Fake: {acc_fake*100}\")\n",
    "\n",
    "# Assuming train_data_reshaped is your input data\n",
    "n_timesteps = train_data_reshaped.shape[1] #\n",
    "n_features  = train_data_reshaped.shape[2] # \n",
    "data_shape = (n_timesteps, n_features)\n",
    "# Assuming 'dataset' is your loaded well log data and it's preprocessed (e.g., scaled)\n",
    "latent_dim = 100 # This is an example; adjust based on your specific needs\n",
    "# Build the models\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "#TODO: colocar as dimensoes como entrada, remover hardcoded\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "gan.summary()\n",
    "\n",
    "# Train GAN\n",
    "train_gan(generator, discriminator, gan, train_data_reshaped, y_train_encoded, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08351141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(generator, latent_dim, n_samples, n_classes):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    sampled_labels = np.random.randint(0, num_classes, n_samples)\n",
    "    sampled_labels = to_categorical(sampled_labels, num_classes=num_classes)\n",
    "    X, y_labels = generator.predict([x_input, sampled_labels])\n",
    "    y_labels = np.argmax(y_labels, axis=-1)\n",
    "    return X, y_labels\n",
    "\n",
    "n_synthetic_samples = 5000  # Number of synthetic samples to generate\n",
    "synthetic_data, synthetic_labels = generate_synthetic_data(generator, latent_dim, n_synthetic_samples, n_classes=len(np.unique(y_train)))\n",
    "#TODO: Undo the reshape of synthetic data (5000, 6, 1) -> (5000, 6)\n",
    "synthetic_data = np.squeeze(synthetic_data, axis=-1)\n",
    "\n",
    "# Combine real and synthetic data\n",
    "X_augmented = np.vstack((X_train, synthetic_data))\n",
    "y_augmented = np.hstack((y_train, synthetic_labels))\n",
    "\n",
    "# Step 4: Train a Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_augmented, y_augmented)\n",
    "\n",
    "# Step 5: Evaluate the Classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1D_model():\n",
    "\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "       \n",
    "    \n",
    "    model = keras.Sequential(name=\"model_conv1D\")\n",
    "    \n",
    "    # 1st layer\n",
    "    ks = 2\n",
    "    model.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_1\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_2\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_3\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_4\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    \n",
    "    #model.add(keras.layers.MaxPooling1D(pool_size=1, name=\"MaxPooling1D_fisrt\"))\n",
    "    \n",
    "    # Dense\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_conv1D = build_conv1D_model()\n",
    "model_conv1D.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84baee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystoping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                patience=5,\n",
    "                                                verbose=1,\n",
    "                                                mode='auto',\n",
    "                                                restore_best_weights=True)\n",
    "checkpoint_filepath = 'weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      monitor='val_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model_conv1D.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'])\n",
    "plt.plot(history_cnn.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cnn = model_conv1D.predict(test_data_reshaped)\n",
    "pred_test_cnn = tf.argmax(pred_test_cnn, axis=1)\n",
    "test_loss, test_acc = model_conv1D.evaluate(test_data_reshaped,  y_test, verbose=2)\n",
    "print(classification_report(y_test, pred_test_cnn, target_names=training_features))\n",
    "cm_test_cnn = confusion_matrix(y_test, pred_test_cnn)\n",
    "plot_confusion_matrix(cm_test_cnn, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd138124",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_cnn = f1_score(y_test, pred_test_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_cnn)\n",
    "#\n",
    "X_blind_reshaped = X_blind_stnd.reshape(X_blind_stnd.shape[0],X_blind_stnd.shape[1],1)\n",
    "aux = model_conv1D.predict(X_blind_reshaped)\n",
    "pred_blind_cnn = tf.argmax(aux, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2031d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blind CM\n",
    "print(classification_report(y_blind, pred_blind_cnn))\n",
    "cm_cnn = confusion_matrix(y_blind, pred_blind_cnn)\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','NaN','L','NaN','A','T']\n",
    "plot_confusion_matrix(cm_cnn, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_blind_cnn = f1_score(y_blind, pred_blind_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_blind_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbflayer import RBFLayer, InitCentersRandom\n",
    "def build_conv1D_rbf_model():\n",
    "    #\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "    #\n",
    "    model_rbf = keras.Sequential(name=\"model_conv1D_rbf\")\n",
    "    # 1st layer\n",
    "    ks = 1\n",
    "    mp=1\n",
    "    num_filters=128\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_1\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_2\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 2nd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_3\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_4\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 3rd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_5\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=num_filters, kernel_size=ks, activation='relu', name=\"Conv1D_6\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    model_rbf.add(RBFLayer(num_filters,\n",
    "                           #TODO: Implement the function below\n",
    "                        #    initializer=InitCentersRandom(X_blind_reshaped),\n",
    "                           betas=2.0,\n",
    "                           input_shape=(1,)))\n",
    "    model_rbf.add(keras.layers.Flatten())\n",
    "    model_rbf.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model_rbf.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model_rbf\n",
    "\n",
    "model_conv1D_rbf = build_conv1D_rbf_model()\n",
    "model_conv1D_rbf.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rbf = model_conv1D_rbf.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['loss'])\n",
    "plt.plot(history_rbf.history['val_loss'])\n",
    "plt.title('model loss CNN (RBF)')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f913bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['accuracy'])\n",
    "plt.plot(history_rbf.history['val_accuracy'])\n",
    "plt.title('model accuracy CNN (RBF)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee927129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rbf = model_conv1D_rbf.predict(test_data_reshaped)\n",
    "pred_test_rbf = tf.argmax(pred_test_rbf, axis=1)\n",
    "print(classification_report(y_test, pred_test_rbf))\n",
    "cm_test_rbf = confusion_matrix(y_test, pred_test_rbf)\n",
    "plot_confusion_matrix(cm_test_rbf, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_rbf = f1_score(y_test, pred_test_rbf, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_rbf)\n",
    "pred_blind_rbf = model_conv1D_rbf.predict(X_blind_reshaped)\n",
    "pred_blind_rbf = tf.argmax(pred_blind_rbf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_blind, pred_blind_rbf))\n",
    "cm_rbf = confusion_matrix(y_blind, pred_blind_rbf)\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','L','NaN','A','T']\n",
    "plot_confusion_matrix(cm_rbf, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "#\n",
    "def build_lstm_model():\n",
    "    n_timesteps = train_data_reshaped.shape[1]\n",
    "    n_features = train_data_reshaped.shape[2]\n",
    "\n",
    "    model_lstm = keras.Sequential(name=\"model_lstm\")\n",
    "    \n",
    "    # 1st layer\n",
    "    num_lstm_units = 128\n",
    "    model_lstm.add(keras.layers.Input(shape=(n_timesteps, n_features)))\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_1\"))\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_2\"))\n",
    "    model_lstm.add(keras.layers.Dropout(0.2))\n",
    "    model_lstm.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # 2nd layer\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_3\"))\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_4\"))\n",
    "    model_lstm.add(keras.layers.Dropout(0.2))\n",
    "    model_lstm.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # 3rd layer\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_5\"))\n",
    "    model_lstm.add(LSTM(units=num_lstm_units, return_sequences=True, activation='relu', name=\"LSTM_6\"))\n",
    "    model_lstm.add(keras.layers.Dropout(0.2))\n",
    "    model_lstm.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # Dense\n",
    "    model_lstm.add(keras.layers.Flatten())\n",
    "    model_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model_lstm.add(keras.layers.Dropout(0.2))\n",
    "    model_lstm.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_aux, metrics=['accuracy'])\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "model_lstm = build_lstm_model()\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794efec6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.title('model loss LSTM')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb459c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_lstm.history['accuracy'])\n",
    "plt.plot(history_lstm.history['val_accuracy'])\n",
    "plt.title('model accuracy LSTM')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a48089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_lstm = model_lstm.predict(test_data_reshaped)\n",
    "pred_test_lstm = tf.argmax(pred_test_lstm, axis=1)\n",
    "#\n",
    "print(classification_report(y_test, pred_test_lstm))\n",
    "cm_test_lstm = confusion_matrix(y_test, pred_test_lstm)\n",
    "plot_confusion_matrix(cm_test_lstm, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_lstm = f1_score(y_test, pred_test_lstm, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_lstm = model_lstm.predict(X_blind_reshaped)\n",
    "pred_blind_lstm = tf.argmax(pred_blind_lstm, axis=1)\n",
    "print(classification_report(y_blind, pred_blind_lstm))\n",
    "cm_lstm = confusion_matrix(y_blind, pred_blind_lstm)\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','NaN','L','NaN','A','T','NaN']\n",
    "plot_confusion_matrix(cm_lstm, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8416f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, MaxPooling1D, Dropout, BatchNormalization, Flatten, Dense\n",
    "\n",
    "def build_gru_model():\n",
    "    n_timesteps = train_data_reshaped.shape[1]\n",
    "    n_features = train_data_reshaped.shape[2]\n",
    "    \n",
    "    model_gru = Sequential(name=\"model_gru\")\n",
    "    \n",
    "    # GRU Layers\n",
    "    num_units = 128\n",
    "    model_gru.add(Input(shape=(n_timesteps, n_features)))\n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_1\"))\n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_2\"))\n",
    "    model_gru.add(MaxPooling1D(pool_size=1))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(BatchNormalization())\n",
    "    \n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_3\"))\n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_4\"))\n",
    "    model_gru.add(MaxPooling1D(pool_size=1))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(BatchNormalization())\n",
    "    \n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_5\"))\n",
    "    model_gru.add(GRU(units=num_units, return_sequences=True, activation='relu', name=\"GRU_6\"))\n",
    "    model_gru.add(MaxPooling1D(pool_size=1))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(BatchNormalization())\n",
    "    \n",
    "    # Flatten Layer\n",
    "    model_gru.add(Flatten())\n",
    "    \n",
    "    # Dense Layers\n",
    "    model_gru.add(Dense(512, activation='relu'))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    # Model Compilation\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model_gru.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_aux, metrics=['accuracy'])\n",
    "    \n",
    "    return model_gru\n",
    "\n",
    "model_gru = build_gru_model()\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = model_gru.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d758c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_gru.history['loss'])\n",
    "plt.plot(history_gru.history['val_loss'])\n",
    "plt.title('model loss GRU')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166006fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history_gru.history['accuracy'])\n",
    "plt.plot(history_gru.history['val_accuracy'])\n",
    "plt.title('model accuracy GRU')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_gru = model_gru.predict(test_data_reshaped)\n",
    "pred_test_gru = tf.argmax(pred_test_gru, axis=1)\n",
    "print(classification_report(y_test, pred_test_gru))\n",
    "cm_test_gru = confusion_matrix(y_test, pred_test_gru)\n",
    "plot_confusion_matrix(cm_test_gru, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02052ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_gru = f1_score(y_test, pred_test_gru, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f395085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_gru = model_gru.predict(X_blind_reshaped)\n",
    "pred_blind_gru = tf.argmax(pred_blind_gru, axis=1)\n",
    "print(classification_report(y_blind, pred_blind_gru))\n",
    "cm_gru = confusion_matrix(y_blind, pred_blind_gru)\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','NaN','L','NaN','A','T','NaN']\n",
    "plot_confusion_matrix(cm_gru, list_blind_full, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d64c16",
   "metadata": {},
   "source": [
    "# 10. Model performance evaluation\n",
    "\n",
    "I will use the diagnosis of confusion matrix from train data set to evaluate the model performance. The diagnosis of confusion matrix points how much percentage of the stone is correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of \n",
    "### facies for each machine learning algorithm\n",
    "\n",
    "mod_test_list = ['CNN','CNN-RBF','LSTM','GRU']\n",
    "cm_test_list = [cm_test_cnn, cm_test_rbf, cm_test_lstm, cm_test_gru]\n",
    "face_test_list = training_features\n",
    "pred_test_df = pd.DataFrame(index=training_features, columns=mod_test_list)\n",
    "\n",
    "for mod in mod_test_list:\n",
    "    \n",
    "    col_index = int(mod_test_list.index(mod))\n",
    "    cm = cm_test_list[col_index]\n",
    "    \n",
    "    for face in face_test_list:\n",
    "        row_index = training_features.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_test_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "        \n",
    "\n",
    "### add the accuracy factor\n",
    "df_1 = pd.DataFrame([[microF1_test_cnn, \n",
    "                      microF1_test_rbf,\n",
    "                      microF1_test_lstm,\n",
    "                      microF1_test_gru]], index=['Accuracy'], columns=mod_test_list)    \n",
    "\n",
    "\n",
    "pred_test_conc = pd.concat([pred_test_df,df_1])\n",
    "pred_test_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47632fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_test_df.shape[0])\n",
    "(pred_df_index_list) = training_features\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_test_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_test_df['CNN-RBF'], color='red', width=aux)\n",
    "plt.bar(X_ind+0.2, pred_test_df['LSTM'], color='green', width=aux)\n",
    "plt.bar(X_ind+0.3, pred_test_df['GRU'], color='black', width=aux)\n",
    "\n",
    "plt.xticks(X_ind, pred_df_index_list)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_test_list)\n",
    "plt.savefig('canada_performance_evaluation_test_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c254c6",
   "metadata": {},
   "source": [
    "# 11. Calssifier evluation using blind test well\n",
    "\n",
    "I will use the same method shown in item4 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of facies of blind test well for each machine learning algorithm\n",
    "\n",
    "# blind_class  = ['Sandstone',\n",
    "#                   'Sandstone/Shale',\n",
    "#                   'Shale',\n",
    "#                   'Marl',\n",
    "#                   'Limestone',\n",
    "#                   'Chalk',\n",
    "#                   'Anhydrite',\n",
    "#                   'Tuff']\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','L','A','NaN','T']\n",
    "mod_list = ['CNN','CNN-RBF','LSTM','GRU']\n",
    "cm_list = [cm_cnn, cm_rbf, cm_lstm, cm_gru]\n",
    "pred_df = pd.DataFrame(index=list_blind_full, columns=mod_list)\n",
    "\n",
    "for mod in mod_list:\n",
    "    col_index = int(mod_list.index(mod))\n",
    "    cm = cm_list[col_index]\n",
    "    \n",
    "    for face in list_blind_full:\n",
    "        \n",
    "        row_index = list_blind_full.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1affc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_df.shape[0])\n",
    "\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_df['CNN-RBF'], color='red', width=aux)\n",
    "plt.bar(X_ind+0.3, pred_df['LSTM'], color='green', width=aux)\n",
    "plt.bar(X_ind+0.4, pred_df['GRU'], color='black', width=aux)\n",
    "plt.xticks(X_ind, list_blind_full)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_list)\n",
    "plt.savefig('canada_performance_evaluation_blind_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe72fa",
   "metadata": {},
   "source": [
    "# 12. Plot the predicted facies for comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = blind.copy()\n",
    "blind['CNN'] = pred_blind_cnn\n",
    "blind['CNN-RBF'] = pred_blind_rbf\n",
    "blind['LSTM'] = pred_blind_lstm\n",
    "blind['GRU'] = pred_blind_gru\n",
    "\n",
    "\n",
    "blind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "facies_colors = ['bisque',\n",
    "                 'darkorange',\n",
    "                 'darkgoldenrod',\n",
    "                 'peachpuff',\n",
    "                 'beige',\n",
    "                 'white',\n",
    "                 'red']\n",
    "\n",
    "blind_class  = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                  'L',\n",
    "                  'A',\n",
    "                  'T']\n",
    "\n",
    "def compare_facies_plot(logs, facie1, facie2, facie3, facie4, facies_colors):\n",
    "    #make sure logs are sorted by depth\n",
    "    logs = logs.sort_values(by='DEPTH_MD')\n",
    "    cmap_facies = colors.ListedColormap(facies_colors[0:len(facies_colors)], 'indexed')\n",
    "    num_colors = 7\n",
    "    ztop=logs.DEPTH_MD.min(); zbot=logs.DEPTH_MD.max()\n",
    "    #\n",
    "    cluster0 = np.repeat(np.expand_dims(logs['LITH_SI'].values,1), 100, 1)\n",
    "    cluster1 = np.repeat(np.expand_dims(logs[facie1].values,1), 100, 1)\n",
    "    cluster2 = np.repeat(np.expand_dims(logs[facie2].values,1), 100, 1)\n",
    "    cluster3 = np.repeat(np.expand_dims(logs[facie3].values,1), 100, 1)\n",
    "    cluster4 = np.repeat(np.expand_dims(logs[facie4].values,1), 100, 1)\n",
    "    # cluster5 = np.repeat(np.expand_dims(logs[compare5].values,1), 100, 1)\n",
    "    # cluster6 = np.repeat(np.expand_dims(logs[compare6].values,1), 100, 1)\n",
    "    # cluster7 = np.repeat(np.expand_dims(logs[compare7].values,1), 100, 1)\n",
    "    #\n",
    "    f, ax = plt.subplots(nrows=1, ncols=11, figsize=(18, 15))\n",
    "    ax[0].plot(logs.CALI, logs.DEPTH_MD, '-',color='red')\n",
    "    ax[1].plot(logs.GR, logs.DEPTH_MD, '-',color='blue')\n",
    "    ax[2].plot(logs.NPHI, logs.DEPTH_MD, '-', color='red')\n",
    "    ax[3].plot(np.log10(logs.RDEP), logs.DEPTH_MD, '-', color='green')\n",
    "    ax[4].plot(logs.RHOB, logs.DEPTH_MD, '--', color='blue')\n",
    "    ax[5].plot(np.log10(logs.RMED), logs.DEPTH_MD, '-', color='black')\n",
    "    # ax[6].plot(logs.PEF, logs.DEPTH_MD, '-', color='black')\n",
    "    im0 = ax[6].imshow(cluster0, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im1 = ax[7].imshow(cluster1, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im2 = ax[8].imshow(cluster2, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im3 = ax[9].imshow(cluster3, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im4 = ax[10].imshow(cluster4, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[12].imshow(cluster5, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[13].imshow(cluster6, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[14].imshow(cluster7, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    \n",
    "            \n",
    "    divider = make_axes_locatable(ax[10])\n",
    "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
    "    cbar=plt.colorbar(im4, cax=cax)\n",
    "    cbar.set_label((30*' ').join(blind_class))\n",
    "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
    "    \n",
    "    for i in range(len(ax)-5): # 5 represents facies number\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=5)\n",
    "    \n",
    "    ax[0].set_xlabel(\"CALI\")\n",
    "    ax[0].set_xlim(8,16)\n",
    "    \n",
    "    ax[1].set_xlabel(\"GR\")\n",
    "    ax[1].set_xlim(0,200)\n",
    "    \n",
    "    ax[2].set_xlabel(\"NPHI\")\n",
    "    ax[2].set_xlim(0.45,-0.15)\n",
    "    \n",
    "    ax[3].set_xlabel(\"RDEP\")\n",
    "    # ax[3].set_xlim(0.2,200)\n",
    "    \n",
    "    ax[4].set_xlabel(\"RHOB\")\n",
    "    ax[4].set_xlim(1.95,2.95)\n",
    "    \n",
    "    ax[5].set_xlabel(\"RMED\")\n",
    "    # ax[5].set_xlim(0.2,200)\n",
    "    \n",
    "    # ax[6].set_xlabel(\"PEF\")\n",
    "    # ax[6].set_xlim(logs.PEF.min(),logs.PEF.max())\n",
    "    \n",
    "    ax[6].set_xlabel('Facies')\n",
    "    ax[7].set_xlabel(facie1)\n",
    "    ax[8].set_xlabel(facie2)\n",
    "    ax[9].set_xlabel(facie3)\n",
    "    ax[10].set_xlabel(facie4)\n",
    "    # ax[12].set_xlabel(compare5)\n",
    "    # ax[13].set_xlabel(compare6)\n",
    "    # ax[14].set_xlabel(compare7)\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    ax[4].set_yticklabels([]); ax[5].set_yticklabels([]); ax[6].set_yticklabels([])\n",
    "    ax[7].set_yticklabels([]); ax[8].set_yticklabels([]); ax[9].set_yticklabels([])\n",
    "    ax[10].set_yticklabels([]); #ax[11].set_yticklabels([])#; ax[12].set_yticklabels([])\n",
    "    # ax[13].set_yticklabels([]); ax[14].set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    ax[5].set_xticklabels([])\n",
    "    ax[6].set_xticklabels([])\n",
    "    ax[7].set_xticklabels([])\n",
    "    ax[8].set_xticklabels([])\n",
    "    ax[9].set_xticklabels([])\n",
    "    ax[10].set_xticklabels([])\n",
    "    #ax[11].set_xticklabels([])\n",
    "    # ax[12].set_xticklabels([])\n",
    "    # ax[13].set_xticklabels([])\n",
    "    # ax[14].set_xticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['WELL'], fontsize=14,y=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_facies_plot(blind, 'CNN','CNN-RBF', 'LSTM', 'GRU', facies_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
