{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b570a7",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "filename = 'NorthSeaData/FORCE_2020_train.csv' # nome do dado de entrada\n",
    "df = pd.read_csv(filename) # leitura do dado de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca095dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### number feature (well log profiling) visualization\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata \n",
    "training_data = df[['WELL', \n",
    "                    'DEPTH_MD', \n",
    "                    'RMED', \n",
    "                    'RDEP', \n",
    "                    'RHOB', \n",
    "                    'GR', \n",
    "                    'NPHI',\n",
    "                    'DTC', \n",
    "                    'PEF', \n",
    "                    'FORCE_2020_LITHOFACIES_LITHOLOGY']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata to use \n",
    "df = df[['WELL', \n",
    "         'DEPTH_MD', \n",
    "         'RMED', \n",
    "         'RDEP', \n",
    "         'RHOB', \n",
    "         'GR', \n",
    "         'NPHI',\n",
    "         'DTC', \n",
    "         'PEF', \n",
    "         'FORCE_2020_LITHOFACIES_LITHOLOGY']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccee374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the number of wells\n",
    "for well in df['WELL'].unique():\n",
    "    \n",
    "    print(well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ba12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of headers\n",
    "plot_cols = ['WELL', 'DEPTH_MD','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI',\n",
    "             'DTC', 'PEF','FORCE_2020_LITHOFACIES_LITHOLOGY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[plot_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, col in enumerate(data_nan.columns[2:]):\n",
    "    data_nan[col] = data_nan[col].notnull() * (num + 1)\n",
    "    data_nan[col].replace(0, num, inplace=True)\n",
    "    print(col, num) #Print out the col name and number to verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96554fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6682d7",
   "metadata": {},
   "source": [
    "# 2. Plotting the Data with and without NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_nan.groupby('WELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f455771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the labels we want to display on the x-axis\n",
    "#labels = ['RMED','RDEP', 'RHOB','GR', 'NPHI', 'DTC'] # 6 features\n",
    "\n",
    "#labels = ['CALI','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF','DTC', 'SP','DTS','DRHO', 'RMIC','RXO'] # 13 features\n",
    "labels = ['RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "#Setup the figure and the subplots\n",
    "fig, axs = plt.subplots(3, 4, figsize=(20,10))\n",
    "\n",
    "#Loop through each well and column in the grouped dataframe\n",
    "for (name, df), ax in zip(grouped, axs.flat):\n",
    "    #ax.set_xlim(0,5) # 6 features\n",
    "    ax.set_xlim(0,6) # 9 features\n",
    "    \n",
    "    #Setup the depth range\n",
    "    ax.set_ylim(4000, 0)\n",
    "    \n",
    "    #Create multiple fill betweens for each curve# This is between\n",
    "    # the number representing null values and the number representing\n",
    "    # actual values\n",
    "    \n",
    "    #ax.fill_betweenx(df.DEPTH_MD, 0, df.CALI, facecolor='grey')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 0, df.RMED, facecolor='lightgrey')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 1, df.RDEP, facecolor='mediumseagreen')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 2, df.RHOB, facecolor='lightblue')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 3, df.GR, facecolor='lightcoral')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 4, df.NPHI, facecolor='violet')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 5, df.DTC, facecolor='darksalmon')\n",
    "    ax.fill_betweenx(df.DEPTH_MD, 6, df.PEF, facecolor='red')\n",
    "    #ax.fill_betweenx(df.DEPTH_MD, 6, df.SP, facecolor='thistle')\n",
    "  \n",
    "    \n",
    "    #Setup the grid, axis labels and ticks\n",
    "    ax.grid(axis='x', alpha=0.5, color='black')\n",
    "    ax.set_ylabel('DEPTH (m)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    #Position vertical lines at the boundaries between the bars\n",
    "    ax.set_xticks([1,2,3,4,5,6,7], minor=False)\n",
    "    \n",
    "    #Position the curve names in the centre of each column\n",
    "    ax.set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5], minor=True)\n",
    "    \n",
    "    #Setup the x-axis tick labels\n",
    "    ax.set_xticklabels(labels,  rotation='vertical', minor=True, verticalalignment='bottom')\n",
    "    ax.set_xticklabels('', minor=False)\n",
    "    ax.tick_params(axis='x', which='minor', pad=-7)\n",
    "    \n",
    "    #Assign the well name as the title to each subplot\n",
    "    ax.set_title(name, fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.savefig('missingdata_northsea.pdf')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4cd02",
   "metadata": {},
   "source": [
    "# 3. Select the headers to use in the in-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aabda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY':'FACIES'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55db76c",
   "metadata": {},
   "source": [
    "# 4. Column Remapping / Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16818e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_numbers = {30000: 'Sandstone', # sandybrown\n",
    "                     65030: 'Sandstone/Shale', #darkgoldenrod\n",
    "                     65000: 'Shale', # olive\n",
    "                     80000: 'Marl', #gainsboro\n",
    "                     74000: 'Dolomite',\n",
    "                     70000: 'Limestone',\n",
    "                     70032: 'Chalk',\n",
    "                     88000: 'Halite',\n",
    "                     86000: 'Anhydrite',\n",
    "                     99000: 'Tuff',\n",
    "                     90000: 'Coal',\n",
    "                     93000: 'Basement'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3945815",
   "metadata": {},
   "source": [
    "second dictionary to tranform in integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275513ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lithology_numbers = {30000: 1,\n",
    "                            65030: 2,\n",
    "                            65000: 3,\n",
    "                            80000: 4,\n",
    "                            74000: 5,\n",
    "                            70000: 6,\n",
    "                            70032: 7,\n",
    "                            88000: 8,\n",
    "                            86000: 9,\n",
    "                            99000: 10,\n",
    "                            90000: 11,\n",
    "                            93000: 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e06db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['LITH'] = training_data['FACIES'].map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb272051",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['LITH_SI'] = training_data['FACIES'].map(simple_lithology_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e964b7d",
   "metadata": {},
   "source": [
    "# 5. View the number of samples of the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24464aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "training_data['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(training_data['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,11,1)\n",
    "plt.title('Number of samples')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Dolomite',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Halite',\n",
    "                  'Anhydrite',\n",
    "                  'Tuff',\n",
    "                  'Coal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a4bf9",
   "metadata": {},
   "source": [
    "# 6. Crossplot RHOB and NPHI (whole data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce874c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.FacetGrid(training_data, col='LITH', col_wrap=4)\n",
    "g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)\n",
    "g.set(xlim=(-0.15, 1))\n",
    "g.set(ylim=(3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23107df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN\n",
    "training_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd218ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for well in training_data['WELL'].unique():\n",
    "    \n",
    "    print(well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57943888",
   "metadata": {},
   "source": [
    "# 7. sorting out the blind test well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38febc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = training_data[training_data['WELL'] == '16/2-16'] #seleciona um poço apenas do dado\n",
    "training_data = training_data[training_data['WELL'] != '16/2-16'] #remove o poço do dado\n",
    "blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['WELL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.FacetGrid(training_data, col='LITH', col_wrap=4)\n",
    "g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)\n",
    "g.set(xlim=(-0.15, 1))\n",
    "g.set(ylim=(3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.FacetGrid(blind, col='LITH', col_wrap=4)\n",
    "g.map(sns.scatterplot, 'NPHI', 'RHOB', alpha=0.5)\n",
    "g.set(xlim=(-0.15, 1))\n",
    "g.set(ylim=(3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d45614",
   "metadata": {},
   "source": [
    "Two lithofacoes are exluded from data after dropping NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "blind['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(blind['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,7,1)\n",
    "plt.title('Samples - Blind well')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Limestone',\n",
    "                  'Anhydrit',\n",
    "                  'Tuff'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62404bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['WELL', 'DEPTH_MD', 'RDEP', 'RHOB','GR', 'NPHI', 'PEF', 'DTC','SP']\n",
    "#col_list = ['LITH_SI','RDEP', 'RHOB','GR', 'NPHI', 'PEF', 'DTC','SP']\n",
    "\n",
    "col_list = ['LITH_SI','RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "i=0\n",
    "for col in col_list:\n",
    "    i+=1\n",
    "    plt.subplot(3,4,i)\n",
    "    plt.hist(training_data[col])\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of Facies\n",
    "training_data['LITH_SI'].value_counts().sort_index().plot(kind='bar')\n",
    "print(training_data['LITH_SI'].value_counts().sort_index())\n",
    "X_ind = np.arange(0,11,1)\n",
    "plt.title('Samples - Training wells')\n",
    "plt.xticks(X_ind,['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Dolomite',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Halite',\n",
    "                  'Anhydrite',\n",
    "                  'Tuff',\n",
    "                  'Coal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2993d2e",
   "metadata": {},
   "source": [
    "# 8. Prepare data for modeling and blind test well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29480bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RMED', 'RDEP', 'RHOB', 'GR', 'NPHI','DTC', 'PEF']\n",
    "\n",
    "\n",
    "\n",
    "y = training_data['LITH_SI']\n",
    "X = training_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ab811",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data for modelling\n",
    "\n",
    "#scaler = StandardScaler().fit(X)\n",
    "#X_stnd = scaler.transform(X)\n",
    "\n",
    "# standarization of data for SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f55e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eca1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blind test well\n",
    "\n",
    "y_blind = blind['LITH_SI']\n",
    "X_blind = blind[features]\n",
    "X_blind_stnd = sc.transform(X_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e16f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss and accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1000$]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mae']), label='Train')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mae']),label = 'Val')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,max(history.history['val_mae'])])\n",
    "\n",
    "def plot_prediction(test_labels, test_predictions):\n",
    "    plt.figure()\n",
    "    plt.scatter(test_labels, test_predictions)\n",
    "    plt.xlabel('True Values [1000$]')\n",
    "    plt.ylabel('Predictions [1000$]')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(plt.xlim())\n",
    "    plt.ylim(plt.ylim())\n",
    "    _ = plt.plot([-100, 100],[-100,100])\n",
    "\n",
    "    plt.figure()\n",
    "    error = test_predictions - test_labels\n",
    "    plt.hist(error, bins = 50)\n",
    "    plt.xlabel(\"Prediction Error [1000$]\")\n",
    "    _ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greys):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "\n",
    "    if normalize:\n",
    "        \n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb3962",
   "metadata": {},
   "source": [
    "# 9. Parameter optimization and classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC # To use Support Vector Machine\n",
    "from sklearn import ensemble # To use Gradient Boosting and Random forest\n",
    "# from sklearn.neighbors import KNeighborsClassifier # To use KNN\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = ['Ss','Ss/Sh','Sh','M','D','L','Ch','H','T','C']\n",
    "# list_blind_full =   ['Ss','Ss/Sh','Sh','M','L','Ch','A','T','C']\n",
    "list_blind_full =   ['Ss','Ss/Sh','Sh','M','L','A','T']\n",
    "# why do not fit and transform  GRADIENT BOOST\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8b48b",
   "metadata": {},
   "source": [
    "### 9.5 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[1].shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ae6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_train.shape[0] # number of samples in train set\n",
    "time_steps  = X_train.shape[1] # number of features in train set\n",
    "input_dimension = 1               # each feature is represented by 1 number\n",
    "\n",
    "train_data_reshaped = X_train.reshape(sample_size,time_steps,input_dimension)\n",
    "print(\"After reshape train data set shape:\\n\", train_data_reshaped.shape)\n",
    "print(\"1 Sample shape:\\n\",train_data_reshaped[0].shape)\n",
    "print(\"An example sample:\\n\", train_data_reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1D_model():\n",
    "\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "       \n",
    "    \n",
    "    model = keras.Sequential(name=\"model_conv1D\")\n",
    "    \n",
    "    # 1st layer\n",
    "    ks = 2\n",
    "    model.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_1\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_2\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_3\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    model.add(keras.layers.Conv1D(filters=200, kernel_size=ks, strides=1, padding='valid', activation='relu', name=\"Conv1D_4\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=1))\n",
    "    \n",
    "    #model.add(keras.layers.MaxPooling1D(pool_size=1, name=\"MaxPooling1D_fisrt\"))\n",
    "    \n",
    "    # Dense\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_conv1D = build_conv1D_model()\n",
    "model_conv1D.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84baee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystoping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                patience=5,\n",
    "                                                verbose=1,\n",
    "                                                mode='auto',\n",
    "                                                restore_best_weights=True)\n",
    "checkpoint_filepath = 'weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      monitor='val_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model_conv1D.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'])\n",
    "plt.plot(history_cnn.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cnn = model_conv1D.predict(test_data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_cnn = tf.argmax(pred_test_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_conv1D.evaluate(test_data_reshaped,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test_cnn, target_names=training_features))\n",
    "cm_test_cnn = confusion_matrix(y_test, pred_test_cnn)\n",
    "plot_confusion_matrix(cm_test_cnn, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd138124",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_cnn = f1_score(y_test, pred_test_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blind_reshaped = X_blind_stnd.reshape(X_blind_stnd.shape[0],X_blind_stnd.shape[1],1)\n",
    "X_blind_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = model_conv1D.predict(X_blind_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_cnn = tf.argmax(aux, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2031d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_blind, pred_blind_cnn))\n",
    "cm_cnn = confusion_matrix(y_blind, pred_blind_cnn)\n",
    "plot_confusion_matrix(cm_cnn, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_blind_cnn = f1_score(y_blind, pred_blind_cnn, average='micro')\n",
    "print('Test Macro f1 score:', microF1_blind_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0f4d3",
   "metadata": {},
   "source": [
    "### 9.6 CNN (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Layer\n",
    "# from keras import backend as K\n",
    "\n",
    "# class RBFLayer(Layer):\n",
    "#     def __init__(self, units, gamma, ** kwargs):\n",
    "#         super(RBFLayer, self).__init__( ** kwargs)\n",
    "#         self.units = units\n",
    "#         self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.mu = self.add_weight(name = 'mu',\n",
    "#                                   shape = (int(input_shape[1]), self.units),\n",
    "#                                   initializer = 'uniform',\n",
    "#                                   trainable = True)\n",
    "#         super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         diff = K.expand_dims(inputs) - self.mu\n",
    "#         l2 = K.sum(K.pow(diff, 2), axis = 1)\n",
    "#         res = K.exp(-1 * self.gamma * l2)\n",
    "#         return res\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], self.units)\n",
    "from rbflayer import RBFLayer, InitCentersRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1D_rbf_model():\n",
    "    #\n",
    "    n_timesteps = train_data_reshaped.shape[1] #\n",
    "    n_features  = train_data_reshaped.shape[2] # \n",
    "    #\n",
    "    model_rbf = keras.Sequential(name=\"model_conv1D_rbf\")\n",
    "    # 1st layer\n",
    "    ks = 2\n",
    "    mp=1\n",
    "    f=128\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_1\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_2\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 2nd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_3\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_4\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    # # 3rd layer\n",
    "    model_rbf.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_5\"))\n",
    "    model_rbf.add(keras.layers.Conv1D(filters=f, kernel_size=ks, activation='relu', name=\"Conv1D_6\"))\n",
    "    model_rbf.add(keras.layers.MaxPooling1D(pool_size=mp))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.BatchNormalization())\n",
    "    rbflayer = RBFLayer(f,betas=2.0,input_shape=(1,))\n",
    "    model_rbf.add(rbflayer)\n",
    "    # Dense\n",
    "    model_rbf.add(keras.layers.Flatten())\n",
    "    model_rbf.add(keras.layers.Dense(512, activation='relu'))\n",
    "    model_rbf.add(keras.layers.Dropout(0.2))\n",
    "    model_rbf.add(keras.layers.Dense(12, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer_aux = tf.keras.optimizers.Adam()\n",
    "    model_rbf.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer_aux ,metrics = ['accuracy'])\n",
    "    \n",
    "    return model_rbf\n",
    "\n",
    "model_conv1D_rbf = build_conv1D_rbf_model()\n",
    "model_conv1D_rbf.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rbf = model_conv1D_rbf.fit(train_data_reshaped, y_train, validation_data = (test_data_reshaped,y_test),\n",
    "                           batch_size = 512, \n",
    "                           callbacks = [model_checkpoint,earlystoping],\n",
    "                           epochs = 1000,\n",
    "                           verbose=1)\n",
    "# history_rbf = model_conv1D_rbf.fit(\n",
    "#     train_data_reshaped, \n",
    "#     y_train, \n",
    "#     epochs = 1000,\n",
    "#     # steps_per_epoch=len(train_data_reshaped)/10,\n",
    "#     validation_data = (test_data_reshaped,y_test),\n",
    "#     validation_steps= len(test_data_reshaped),\n",
    "#     batch_size = 512, \n",
    "#     callbacks = [model_checkpoint,earlystoping], \n",
    "    \n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['loss'])\n",
    "plt.plot(history_rbf.history['val_loss'])\n",
    "plt.title('model loss CNN (RBF)')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f913bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rbf.history['accuracy'])\n",
    "plt.plot(history_rbf.history['val_accuracy'])\n",
    "plt.title('model accuracy CNN (RBF)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee927129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rbf = model_conv1D_rbf.predict(test_data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_rbf = tf.argmax(pred_test_rbf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test_rbf))\n",
    "cm_test_rbf = confusion_matrix(y_test, pred_test_rbf)\n",
    "plot_confusion_matrix(cm_test_rbf, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "microF1_test_rbf = f1_score(y_test, pred_test_rbf, average='micro')\n",
    "print('Test Macro f1 score:', microF1_test_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_rbf = model_conv1D_rbf.predict(X_blind_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_blind_rbf = tf.argmax(pred_blind_rbf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_blind = ['Ss',\n",
    "#               'Ss/Sh',\n",
    "#               'Sh',\n",
    "#               'M',\n",
    "#               'L',\n",
    "#               'T']\n",
    "\n",
    "print(classification_report(y_blind, pred_blind_rbf))\n",
    "cm_rbf = confusion_matrix(y_blind, pred_blind_rbf)\n",
    "plot_confusion_matrix(cm_rbf, training_features, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d64c16",
   "metadata": {},
   "source": [
    "# 10. Model performance evaluation\n",
    "\n",
    "I will use the diagnosis of confusion matrix from train data set to evaluate the model performance. The diagnosis of confusion matrix points how much percentage of the stone is correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of \n",
    "### facies for each machine learning algorithm\n",
    "\n",
    "mod_test_list = ['CNN','CNN-RBF']\n",
    "cm_test_list = [cm_test_cnn, cm_test_rbf]\n",
    "face_test_list = training_features\n",
    "pred_test_df = pd.DataFrame(index=training_features, columns=mod_test_list)\n",
    "\n",
    "for mod in mod_test_list:\n",
    "    \n",
    "    col_index = int(mod_test_list.index(mod))\n",
    "    cm = cm_test_list[col_index]\n",
    "    \n",
    "    for face in face_test_list:\n",
    "        row_index = training_features.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_test_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "        \n",
    "\n",
    "### add the accuracy factor\n",
    "df_1 = pd.DataFrame([[microF1_test_cnn, \n",
    "                      microF1_test_rbf]], index=['Accuracy'], columns=mod_test_list)    \n",
    "\n",
    "\n",
    "pred_test_conc = pd.concat([pred_test_df,df_1])\n",
    "pred_test_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47632fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_test_df.shape[0])\n",
    "(pred_df_index_list) = training_features\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_test_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_test_df['CNN-RBF'], color='red', width=aux)\n",
    "\n",
    "plt.xticks(X_ind, pred_df_index_list)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_test_list)\n",
    "plt.savefig('canada_performance_evaluation_test_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c254c6",
   "metadata": {},
   "source": [
    "# 11. Calssifier evluation using blind test well\n",
    "\n",
    "I will use the same method shown in item4 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create a data frame recording the correct prediction (normalized) of facies of blind test well for each machine learning algorithm\n",
    "\n",
    "blind_class  = ['Sandstone',\n",
    "                  'Sandstone/Shale',\n",
    "                  'Shale',\n",
    "                  'Marl',\n",
    "                  'Limestone',\n",
    "                  'Chalk',\n",
    "                  'Anhydrite',\n",
    "                  'Tuff']\n",
    "\n",
    "mod_list = ['CNN','CNN-RBF']\n",
    "cm_list = [cm_cnn, cm_rbf]\n",
    "pred_df = pd.DataFrame(index=list_blind_full, columns=mod_list)\n",
    "\n",
    "for mod in mod_list:\n",
    "    col_index = int(mod_list.index(mod))\n",
    "    cm = cm_list[col_index]\n",
    "    \n",
    "    for face in list_blind_full:\n",
    "        \n",
    "        row_index = list_blind_full.index(face)\n",
    "        #print(face, row_index, col_index)\n",
    "        pred_df.iloc[row_index, col_index] = cm[row_index][row_index]/sum(cm[row_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1affc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ind = np.arange(pred_df.shape[0])\n",
    "\n",
    "aux=0.1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(X_ind, pred_df['CNN'], color='blue', width=aux)\n",
    "plt.bar(X_ind+0.1, pred_df['CNN-RBF'], color='red', width=aux)\n",
    "plt.xticks(X_ind, list_blind_full)\n",
    "plt.xlabel('Facies')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.legend(labels=mod_list)\n",
    "plt.savefig('canada_performance_evaluation_blind_data.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe72fa",
   "metadata": {},
   "source": [
    "# 12. Plot the predicted facies for comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = blind.copy()\n",
    "blind['CNN'] = pred_blind_cnn\n",
    "blind['RBF'] = pred_blind_rbf\n",
    "\n",
    "\n",
    "blind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "facies_colors = ['bisque',\n",
    "                 'darkorange',\n",
    "                 'darkgoldenrod',\n",
    "                 'peachpuff',\n",
    "                 'beige',\n",
    "                 'white',\n",
    "                 'red']\n",
    "\n",
    "blind_class  = ['Ss',\n",
    "                  'Ss/Sh',\n",
    "                  'Sh',\n",
    "                  'M',\n",
    "                  'L',\n",
    "                  'A',\n",
    "                  'T']\n",
    "\n",
    "def compare_facies_plot(logs, compare1, compare2, facies_colors):\n",
    "      #make sure logs are sorted by depth\n",
    "    logs = logs.sort_values(by='DEPTH_MD')\n",
    "    cmap_facies = colors.ListedColormap(\n",
    "            facies_colors[0:len(facies_colors)], 'indexed')\n",
    "    num_colors = 7\n",
    "    ztop=logs.DEPTH_MD.min(); zbot=logs.DEPTH_MD.max()\n",
    "    \n",
    "    cluster0 = np.repeat(np.expand_dims(logs['LITH_SI'].values,1), 100, 1)\n",
    "    cluster1 = np.repeat(np.expand_dims(logs[compare1].values,1), 100, 1)\n",
    "    cluster2 = np.repeat(np.expand_dims(logs[compare2].values,1), 100, 1)\n",
    "    # cluster3 = np.repeat(np.expand_dims(logs[compare3].values,1), 100, 1)\n",
    "    # cluster4 = np.repeat(np.expand_dims(logs[compare4].values,1), 100, 1)\n",
    "    # cluster5 = np.repeat(np.expand_dims(logs[compare5].values,1), 100, 1)\n",
    "    # cluster6 = np.repeat(np.expand_dims(logs[compare6].values,1), 100, 1)\n",
    "    # cluster7 = np.repeat(np.expand_dims(logs[compare7].values,1), 100, 1)\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=10, figsize=(18, 15))\n",
    "    ax[0].plot(logs.RMED, logs.DEPTH_MD, '-',color='red')\n",
    "    ax[1].plot(logs.RDEP, logs.DEPTH_MD, '-',color='blue')\n",
    "    ax[2].plot(logs.RHOB, logs.DEPTH_MD, '-', color='red')\n",
    "    ax[3].plot(logs.GR, logs.DEPTH_MD, '-', color='green')\n",
    "    ax[4].plot(logs.NPHI, logs.DEPTH_MD, '--', color='blue')\n",
    "    ax[5].plot(logs.DTC, logs.DEPTH_MD, '-', color='black')\n",
    "    ax[6].plot(logs.PEF, logs.DEPTH_MD, '-', color='black')\n",
    "    im0 = ax[7].imshow(cluster0, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im1 = ax[8].imshow(cluster1, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    im2 = ax[9].imshow(cluster2, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im3 = ax[10].imshow(cluster3, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[11].imshow(cluster4, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[12].imshow(cluster5, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[13].imshow(cluster6, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    # im4 = ax[14].imshow(cluster7, interpolation='none', aspect='auto',\n",
    "    #                 cmap=cmap_facies,vmin=1,vmax=num_colors)\n",
    "    \n",
    "            \n",
    "    divider = make_axes_locatable(ax[9])\n",
    "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
    "    cbar=plt.colorbar(im2, cax=cax)\n",
    "    cbar.set_label((30*' ').join(blind_class))\n",
    "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
    "    \n",
    "    for i in range(len(ax)-3):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=4)\n",
    "    \n",
    "    ax[0].set_xlabel(\"RMED\")\n",
    "    ax[0].set_xlim(0.2,5)\n",
    "    \n",
    "    ax[1].set_xlabel(\"RDEP\")\n",
    "    ax[1].set_xlim(0.2,5)\n",
    "    \n",
    "    ax[2].set_xlabel(\"RHOB\")\n",
    "    ax[2].set_xlim(1.95,2.95)\n",
    "    \n",
    "    ax[3].set_xlabel(\"GR\")\n",
    "    ax[3].set_xlim(0,150)\n",
    "    \n",
    "    ax[4].set_xlabel(\"NPHI\")\n",
    "    ax[4].set_xlim(0.45,-0.15)\n",
    "    \n",
    "    ax[5].set_xlabel(\"DTC\")\n",
    "    ax[5].set_xlim(logs.DTC.min(),logs.DTC.max())\n",
    "    \n",
    "    ax[6].set_xlabel(\"PEF\")\n",
    "    ax[6].set_xlim(logs.PEF.min(),logs.PEF.max())\n",
    "    \n",
    "    ax[7].set_xlabel('Facies')\n",
    "    ax[8].set_xlabel(compare1)\n",
    "    ax[9].set_xlabel(compare2)\n",
    "    # ax[10].set_xlabel(compare3)\n",
    "    # ax[11].set_xlabel(compare4)\n",
    "    # ax[12].set_xlabel(compare5)\n",
    "    # ax[13].set_xlabel(compare6)\n",
    "    # ax[14].set_xlabel(compare7)\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    ax[4].set_yticklabels([]); ax[5].set_yticklabels([]); ax[6].set_yticklabels([])\n",
    "    ax[7].set_yticklabels([]); ax[8].set_yticklabels([]); ax[9].set_yticklabels([])\n",
    "    # ax[10].set_yticklabels([]); ax[11].set_yticklabels([]); ax[12].set_yticklabels([])\n",
    "    # ax[13].set_yticklabels([]); ax[14].set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    ax[5].set_xticklabels([])\n",
    "    ax[6].set_xticklabels([])\n",
    "    ax[7].set_xticklabels([])\n",
    "    ax[8].set_xticklabels([])\n",
    "    ax[9].set_xticklabels([])\n",
    "    # ax[10].set_xticklabels([])\n",
    "    # ax[11].set_xticklabels([])\n",
    "    # ax[12].set_xticklabels([])\n",
    "    # ax[13].set_xticklabels([])\n",
    "    # ax[14].set_xticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['WELL'], fontsize=14,y=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_facies_plot(blind, 'CNN','RBF', facies_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
